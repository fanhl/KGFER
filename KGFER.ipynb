{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "import numpy as np\n",
    "import random \n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import heapq\n",
    "import pysnooper\n",
    "\n",
    "#from imp import reload\n",
    "#reload(data_loader)  #重新载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aggregator='sum', batch_size_cf=2048, batch_size_kg=2048, dataset='music', dropout=0.1, emb_dim=64, kge_dim=64, l2_weight=1e-07, lr=0.001, mess_dropout='[0.1]', model_path='', n_epochs=30, n_iter=2, neighbor_sample_size=1, node_dropout='[0.1]', ratio=1, regs='[1e-5,1e-5,1e-2]', seed=111, use_att=True, use_kge=True, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# movie\n",
    "parser.add_argument('--model_path', nargs='?', default='',help='Store model path.')\n",
    "parser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\n",
    "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=30, help='the number of epochs')\n",
    "parser.add_argument('--neighbor_sample_size', type=int, default=1, help='the number of neighbors to be sampled')\n",
    "parser.add_argument('--emb_dim', type=int, default=64, help='dimension of user and entity embeddings')\n",
    "parser.add_argument('--kge_dim', type=int, default=64, help='dimension of user and KG embeddings')\n",
    "parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')\n",
    "parser.add_argument('--seed', type=int, default=111, help='random seed')\n",
    "parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',help='Regularization for user and item embeddings.')\n",
    "\n",
    "parser.add_argument('--batch_size_cf', type=int, default=2048,help='CF batch size.')\n",
    "parser.add_argument('--batch_size_kg', type=int, default=2048,help='KG batch size.')\n",
    "\n",
    "parser.add_argument('--use_kge', type=bool, default=True,help='whether using knowledge graph embedding')\n",
    "parser.add_argument('--use_att', type=bool, default=True,help='whether using attention mechanism')   \n",
    "parser.add_argument('--verbose', type=int, default=1,help='Interval of evaluation.')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='keep probability(1-dropout)')\n",
    "parser.add_argument('--mess_dropout', nargs='?', default='[0.1]',\n",
    "                      help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "parser.add_argument('--node_dropout', nargs='?', default='[0.1]',\n",
    "                      help='Keep probability w.r.t. node dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "tf.set_random_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class Fusion(object):\n",
    "    @pysnooper.snoop(prefix='fusion_init')\n",
    "    def __init__(self,emb_dim,neighbor_sample_size):\n",
    "        self.neighbor_sample_size = neighbor_sample_size\n",
    "        self.emb_dim = emb_dim\n",
    "        x = self.neighbor_sample_size\n",
    "        y = self.emb_dim\n",
    "\n",
    "    \n",
    "    @pysnooper.snoop(prefix='mix')\n",
    "    def _fusion_neighbor_entities(self, neighbor_entities, neighbor_relations):\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #pad_numb = self.n_relation - len(neighbor_relations)\n",
    "        #neighbor_relations = np.pad(neighbor_relations,(0,3),'constant',constant_values(0,0))\n",
    "        # 第一个维度(行）前面扩展0行，后面扩展3行，第二个维度（列)不变\n",
    "        #d = np.pad(d,((0,3),(0,0)),'constant',constant_values=(0,0))\n",
    "\n",
    "\n",
    "        #  [batch_size,  n_neighbor, dim] * [batch_size,  n_neighbor, dim]--> [batch_size, n_neighbor,1]---\n",
    "        #user_relation_scores = tf.reduce_mean(neighbor_vectors * neighbor_relations, axis=-1)---\n",
    "        \n",
    "        #[batch_size,neighbor_sample_size,dim]--->[batch_size,neighbor_sample_size]\n",
    "        #_relation_scores = tf.reduce_mean(neighbor_relations, axis=-1)\n",
    "        _relation_scores = tf.matmul(tf.multiply(neighbor_relations,neighbor_entities), self.weights) + self.bias\n",
    "        _relation_scores = tf.squeeze(_relation_scores, axis=2)\n",
    "         # [batch_size,  neighbor_sample_size] --> [batch_size, neighbor_sample_size]\n",
    "        _relation_scores_normalized = tf.nn.softmax(_relation_scores, dim=-1)\n",
    "\n",
    "        # [batch_size, neighbor_sample_size] --> [batch_size, neighbor_sample_size, 1]\n",
    "        _relation_scores_normalized = tf.expand_dims(_relation_scores_normalized, axis=-1)\n",
    "\n",
    "        # 1.[batch_size, neighbor_sample_size, 1]   [batch_size,, neighbor_sample_size, dim]--> [batch_size, neighbor_sample_size, dim]\n",
    "        # 2.[batch_size, neighbor_sample_size, dim] ---> [batch_size, dim]\n",
    "        #neighbors_fusion = tf.reduce_mean(_relation_scores_normalized * neighbor_entities, axis=1)\n",
    "        neighbors_fusion = _relation_scores_normalized * neighbor_entities\n",
    "\n",
    "        conv1D = tf.keras.layers.Conv1D(self.emb_dim,self.neighbor_sample_size,self.neighbor_sample_size,padding='valid')\n",
    "        neighbors_fusion = conv1D(neighbors_fusion)\n",
    "        #conv1d = tf.keras.layers.Conv1D(1,1,1,padding='valid')\n",
    "        #max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=10,strides=10,padding='valid')\n",
    "        #neighbors_fusion = max_pool_1d(neighbors_fusion)\n",
    "        neighbors_fusion = tf.squeeze(neighbors_fusion,axis=1)\n",
    "        #neighbors_fusion = tf.nn.softmax(tf.reduce_sum(_relation_scores_normalized * neighbor_entities, axis=2),dim=1)\n",
    "        #neighbors_fusion = tf.nn.softmax(_relation_scores_normalized * neighbor_entities,dim=1)\n",
    "        #neighbors_fusion = tf.reduce_max(_relation_scores_normalized * neighbor_entities, axis=1)\n",
    "        \n",
    "        return neighbors_fusion\n",
    "\n",
    "    def __call__(self,neighbor_entities, neighbor_relations,user_embeddings):\n",
    "        new_embedding = self.fusion(neighbor_entities, neighbor_relations, user_embeddings)\n",
    "        return new_embedding\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fusion(self,neighbor_entities, neighbor_relations,user_embeddings):\n",
    "        # dimension:\n",
    "        # neighbor_vectors: [batch_size, neighbor_sample_size, n_neighbor, dim]\n",
    "        # neighbor_relations: [batch_size, -neighbor_sample_size, dim]\n",
    "        pass\n",
    "\n",
    "class SumFusion(Fusion):\n",
    "    def __init__(self, emb_dim,neighbor_sample_size,dropout=0.1, act=tf.nn.relu):\n",
    "        #self.dropout = dropout\n",
    "        super(SumFusion, self).__init__(emb_dim,neighbor_sample_size)\n",
    "        self.act = act\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        initializer = tf.keras.initializers.glorot_normal()\n",
    "        self.weights = tf.get_variable(shape=[self.emb_dim, 1], initializer=tf.keras.initializers.glorot_normal(),name='weights1')\n",
    "        self.bias = tf.get_variable(shape=[1], initializer=tf.zeros_initializer(), name='bias1')\n",
    "        \n",
    "        self.weights2 = tf.get_variable(shape=[self.emb_dim*2, self.emb_dim], initializer=tf.keras.initializers.glorot_normal(),name='weights2')\n",
    "        self.bias2 = tf.get_variable(shape=[self.emb_dim], initializer=tf.zeros_initializer(), name='bias2')\n",
    "    @pysnooper.snoop(prefix='sum')\n",
    "    def fusion(self, neighbor_entities, neighbor_relations, user_embeddings):\n",
    "        \n",
    "        neighbors_fusion = self._fusion_neighbor_entities(neighbor_entities, neighbor_relations)\n",
    "\n",
    "        #output = user_embeddings + neighbors_fusion\n",
    "        \n",
    "        output = tf.concat([user_embeddings, neighbors_fusion], axis=-1)\n",
    "        #output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
    "        output = tf.matmul(output, self.weights2) + self.bias2\n",
    "        \n",
    "        # [batch_size, dim]\n",
    "        #output = tf.reshape(output, [self.batch_size, self.dim])\n",
    "\n",
    "        return self.act(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "#@pysnooper.snoop(prefix='RGAT')\n",
    "class RGAT(object):\n",
    "    def __init__(self,data,args,pretrain_data):\n",
    "        self._parse_args(data, args,pretrain_data)\n",
    "        self._build_inputs()\n",
    "        self.weights = self._build_weights()\n",
    "        self._build_model_cf()\n",
    "        self._build_model_kg()\n",
    "        self._build_loss()\n",
    "        \n",
    "        self.items = data[1]\n",
    "        \n",
    "    def _parse_args(self,data, args,pretrain_data):\n",
    "        #n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg,rd\n",
    "        self.n_users = data[0]\n",
    "        self.n_items = data[1]\n",
    "        self.n_entities = data[2]\n",
    "        self.n_relations = data[3]\n",
    "        #self.train_data = data[4]\n",
    "        #self.eval_data = data[5]\n",
    "        #self.test_data = data[6]\n",
    "        #self.kg = data[7]\n",
    "        #self.rd = data[8]\n",
    "        self.dropout=args.dropout\n",
    "        self.kge_dim = args.kge_dim\n",
    "        self.emb_dim = args.emb_dim\n",
    "        \n",
    "        self.batch_size_kg = args.batch_size_kg\n",
    "        self.batch_size_cf = args.batch_size_cf\n",
    "        \n",
    "        self.regs = eval(args.regs)\n",
    "        self.lr =args.lr\n",
    "        \n",
    "        self.pretrain_data = pretrain_data\n",
    "        self.neighbor_sample_size = args.neighbor_sample_size\n",
    "        self.fussion_class = SumFusion\n",
    "\n",
    "        \n",
    "    def _build_inputs(self):\n",
    "        # placeholder definition\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # for knowledge graph modeling (TransD)\n",
    "        #self.A_values = tf.placeholder(tf.float32, shape=[len(self.all_v_list)], name='A_values')\n",
    "\n",
    "        self.h = tf.placeholder(tf.int32, shape=[None], name='h')\n",
    "        self.r = tf.placeholder(tf.int32, shape=[None], name='r')\n",
    "        self.pos_t = tf.placeholder(tf.int32, shape=[None], name='pos_t')\n",
    "        self.neg_t = tf.placeholder(tf.int32, shape=[None], name='neg_t')\n",
    "        # 2 --> (end,relation)\n",
    "        self.neighbors = tf.placeholder(tf.int32,shape=[None,self.neighbor_sample_size,2],name ='neighbours')\n",
    "        # dropout: node dropout (adopted on the ego-networks);\n",
    "        # message dropout (adopted on the convolution operations).\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
    "        \n",
    "    def _build_weights(self):\n",
    "        all_weights = dict()\n",
    "        initializer = tf.keras.initializers.glorot_normal()\n",
    "\n",
    "        if self.pretrain_data is None:\n",
    "            all_weights['user_embed'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embed')\n",
    "            all_weights['entity_embed'] = tf.Variable(initializer([self.n_entities, self.emb_dim]), name='entity_embed')\n",
    "            print('using xavier initialization')\n",
    "        else:\n",
    "            all_weights['user_embed'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embed', dtype=tf.float32)\n",
    "\n",
    "            item_embed = self.pretrain_data['item_embed']\n",
    "            other_embed = initializer([self.n_entities - self.n_items, self.emb_dim])\n",
    "\n",
    "            all_weights['entity_embed'] = tf.Variable(initial_value=tf.concat([item_embed, other_embed], 0),\n",
    "                                                      trainable=True, name='entity_embed', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "\n",
    "        all_weights['relation_embed'] = tf.Variable(initializer([self.n_relations, self.kge_dim]),\n",
    "                                                    name='relation_embed')\n",
    "        all_weights['trans_W'] = tf.Variable(initializer([self.n_relations, self.emb_dim, self.kge_dim]))\n",
    "        \n",
    "        self.cf_weights=tf.Variable(initializer([self.kge_dim*2,1]),name='cf_weights')\n",
    "        self.cf_bias = tf.get_variable(shape=[1], initializer=tf.zeros_initializer(), name='cf_bias')\n",
    "        return all_weights        \n",
    "    \n",
    "    def _build_model_cf(self):\n",
    "        #self.ua_embeddings, self.ea_embeddings = self._create_graphsage_embed()\n",
    "\n",
    "        self.u_e = tf.nn.embedding_lookup(self.weights['user_embed'], self.users)\n",
    "        \n",
    "        self.pos_i_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.pos_items)\n",
    "        self.neg_i_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neg_items)\n",
    "\n",
    "\n",
    "        #加入邻居居信息\n",
    "        self.u_e = self.fusion_neighbor()\n",
    "        #///self.batch_predictions = tf.matmul(self.u_e, self.pos_i_e, transpose_a=False, transpose_b=True)\n",
    "        '''\n",
    "        user_item = tf.concat([self.u_e, self.pos_i_e], axis=-1)\n",
    "        user_item = tf.nn.dropout(user_item, keep_prob=1-self.dropout) \n",
    "        self.batch_predictions = tf.matmul(user_item, self.cf_weights) + self.cf_bias\n",
    "        '''\n",
    "        \n",
    "        self.batch_predictions = tf.reduce_sum(tf.multiply(self.u_e, self.pos_i_e), axis=1)\n",
    "        self.batch_predictions_normalized = tf.sigmoid(self.batch_predictions)\n",
    "    \n",
    "    @pysnooper.snoop(prefix='model')\n",
    "    def fusion_neighbor(self):\n",
    "        #aggregators = []  # store all aggregators\n",
    "        \n",
    "        #neighbors = cdata.construct_neighbours(pos_items)\n",
    "        #print(neighbors[:,:,0])\n",
    "        #print(neighbors[:,:,1])\n",
    "        k = self.neighbors\n",
    "        k_shape = self.neighbors.shape\n",
    "        shape = [self.batch_size_cf,1]\n",
    "        #self.neighbour_e = tf.nn.embedding_lookup(self.weights['entity_embed'], tf.reshape(self.neighbors[:,:,0],shape))\n",
    "        #self.neighbour_r = tf.nn.embedding_lookup(self.weights['relation_embed'], tf.reshape(self.neighbors[:,:,1],shape))\n",
    "        self.neighbour_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neighbors[:,:,0])\n",
    "        self.neighbour_r = tf.nn.embedding_lookup(self.weights['relation_embed'],self.neighbors[:,:,1])\n",
    "        \n",
    "        #relation_vectors = [tf.nn.embedding_lookup(self.relation_emb_matrix, i) for i in relations]\n",
    "        \n",
    "        n_iter = 2\n",
    "        #for i in range(n_iter):\n",
    "            #if i == n_iter - 1:\n",
    "                                                      #batch_size, head,dim, dropout, act, name\n",
    "            #    aggregator = self.aggregator_class(self.batch_size_cf, self.emb_dim, act=tf.nn.tanh)\n",
    "            #else:\n",
    "        fusion = self.fussion_class(self.emb_dim,self.neighbor_sample_size)\n",
    "        \n",
    "        # (self, neighbor_vectors, neighbor_relations, user_embeddings): [batch_size,  n_neighbor, dim]\n",
    "        new_embedding = fusion(\n",
    "                                neighbor_entities=self.neighbour_e,\n",
    "                                neighbor_relations=self.neighbour_r,\n",
    "                                user_embeddings=self.u_e)\n",
    "        #entity_vectors_next_iter.append(vector)\n",
    "           \n",
    "        #res = tf.reshape(vector, [self.batch_size, self.dim])\n",
    "\n",
    "        return new_embedding #, aggregators        \n",
    "    \n",
    "    def _build_model_kg(self):\n",
    "        self.h_e, self.r_e, self.pos_t_e, self.neg_t_e = self._trnasR(self.h, self.r, self.pos_t, self.neg_t)\n",
    "        #self.A_kg_score = self._generate_transE_score(h=self.h, t=self.pos_t, r=self.r)\n",
    "        #self.A_out = self._create_attentive_A_out()       \n",
    "    \n",
    "    def _trnasR(self, h, r, pos_t, neg_t):\n",
    "        \n",
    "        #embeddings = tf.concat([self.weights['user_embed'], self.weights['entity_embed']], axis=0)\n",
    "        # shape (n_entities, 1,emb_dim)\n",
    "        embeddings = tf.expand_dims(self.weights['entity_embed'], 1)\n",
    "\n",
    "        # head & tail entity embeddings: batch_size *1 * emb_dim\n",
    "        h_e = tf.nn.embedding_lookup(embeddings, h)\n",
    "        pos_t_e = tf.nn.embedding_lookup(embeddings, pos_t)\n",
    "        neg_t_e = tf.nn.embedding_lookup(embeddings, neg_t)\n",
    "\n",
    "        # relation embeddings: batch_size * kge_dim\n",
    "        r_e = tf.nn.embedding_lookup(self.weights['relation_embed'], r)\n",
    "\n",
    "        # relation transform weights: batch_size * kge_dim * emb_dim\n",
    "        trans_W = tf.nn.embedding_lookup(self.weights['trans_W'], r)\n",
    "        \n",
    "        #neighbor_relation=dict()\n",
    "        #for i in r:\n",
    "        #    neighbor_relation[i]\n",
    "\n",
    "        '''\n",
    "        (batch_size ,1, emb_dim)*(batch_size,emb_dim,kge_dim) -->(batch_size , 1 , kge_dim)\n",
    "        (batch_size , 1 , kge_dim) -> (batch_size , kge_dim)\n",
    "        '''\n",
    "        h_e = tf.reshape(tf.matmul(h_e, trans_W), [-1, self.kge_dim])\n",
    "        pos_t_e = tf.reshape(tf.matmul(pos_t_e, trans_W), [-1, self.kge_dim])\n",
    "        neg_t_e = tf.reshape(tf.matmul(neg_t_e, trans_W), [-1, self.kge_dim])\n",
    "        \n",
    "        # Remove the l2 normalization terms\n",
    "        # h_e = tf.math.l2_normalize(h_e, axis=1)\n",
    "        # r_e = tf.math.l2_normalize(r_e, axis=1)\n",
    "        # pos_t_e = tf.math.l2_normalize(pos_t_e, axis=1)\n",
    "        # neg_t_e = tf.math.l2_normalize(neg_t_e, axis=1)\n",
    "\n",
    "        return h_e, r_e, pos_t_e, neg_t_e\n",
    "\n",
    "\n",
    "    def _build_loss(self):\n",
    "        pos_scores = tf.reduce_mean(tf.multiply(self.u_e, self.pos_i_e), axis=1)\n",
    "        neg_scores = tf.reduce_mean(tf.multiply(self.u_e, self.neg_i_e), axis=1)\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(self.u_e) + tf.nn.l2_loss(self.pos_i_e) + tf.nn.l2_loss(self.neg_i_e)\n",
    "        regularizer = regularizer / self.batch_size_cf\n",
    "\n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        cf_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
    "        # maxi = tf.log(tf.nn.sigmoid(pos_scores - neg_scores))\n",
    "        # base_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "        self.cf_reg_loss = self.regs[0] * regularizer\n",
    "        \n",
    "        self.cf_loss = cf_loss+self.cf_reg_loss\n",
    "\n",
    "        \n",
    "        \n",
    "        def _get_kg_score(h_e, r_e, t_e):\n",
    "            kg_score = tf.reduce_mean(tf.square((h_e + r_e - t_e)), 1, keepdims=True)\n",
    "            return kg_score\n",
    "\n",
    "        pos_kg_score = _get_kg_score(self.h_e, self.r_e, self.pos_t_e)\n",
    "        neg_kg_score = _get_kg_score(self.h_e, self.r_e, self.neg_t_e)\n",
    "        \n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        kg_loss = tf.reduce_mean(tf.nn.softplus(-(neg_kg_score - pos_kg_score)))\n",
    "        #kg_loss = tf.reduce_mean(tf.nn.softplus(-(pos_kg_score - neg_kg_score)))\n",
    "        #maxi = tf.log(tf.nn.sigmoid(neg_kg_score - pos_kg_score))\n",
    "        #kg_loss = tf.negative(tf.reduce_mean(maxi))\n",
    "\n",
    "\n",
    "        kg_reg_loss = tf.nn.l2_loss(self.h_e) + tf.nn.l2_loss(self.r_e) + \\\n",
    "                      tf.nn.l2_loss(self.pos_t_e) + tf.nn.l2_loss(self.neg_t_e)\n",
    "        kg_reg_loss = kg_reg_loss / self.batch_size_kg\n",
    "\n",
    "        #self.kge_loss2 = kg_loss\n",
    "        self.kg_reg_loss = self.regs[1] * kg_reg_loss\n",
    "        self.kg_loss = kg_loss + self.kg_reg_loss\n",
    "\n",
    "        # Optimization process.\n",
    "        #self.kg_opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.kg_loss)    \n",
    "        \n",
    "        self.loss = 0.8*self.cf_loss + 0.2*self.kg_loss \n",
    "        #self.loss = self.cf_loss + self.cf_reg_loss\n",
    "\n",
    "        # Optimization process.RMSPropOptimizer\n",
    "        self._opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        #return sess.run([self.kg_opt, self.cf_opt,self.loss,self.cf_loss,self.kg_loss,self.kg_reg_loss], feed_dict)\n",
    "        return sess.run([self._opt,self.loss,self.cf_loss,self.cf_reg_loss,self.kg_loss,self.kg_reg_loss], feed_dict)\n",
    "    \n",
    "    #@pysnooper.snoop(prefix='recallK')\n",
    "    def recallK(self,y_true,y_pred,k):\n",
    "        def getTopK(y_pred,k):\n",
    "            topK=[]\n",
    "            a=y_pred\n",
    "            row_len = y_true.shape[0]\n",
    "            col_len = self.items\n",
    "            for i in range(row_len):\n",
    "                b = heapq.nlargest(k,range(col_len),a[i].take)\n",
    "                topK.append(b)\n",
    "            return topK\n",
    "        x=self.items\n",
    "        topK = getTopK(y_pred,k)\n",
    "        print(topK)\n",
    "\n",
    "        T=[]\n",
    "        for i in range(len(y_true)):\n",
    "            b = np.argwhere(y[i]==1)\n",
    "            b = np.squeeze(b)\n",
    "            T.append(b)\n",
    "        print(x)\n",
    "\n",
    "        recall_k = []\n",
    "        for i in range(len(topK)):\n",
    "            #print(topK[i])\n",
    "            #print(T[i])\n",
    "            recall_ = len(set(topK[i])&set(T[i]))/len(T[i])\n",
    "            recall_k.append(recall_)\n",
    "            print(recall/len(topK))\n",
    "            return np.array(recall_k)\n",
    "    \n",
    "    def eval_(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.batch_predictions_normalized], feed_dict)\n",
    "        #AUC\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        \n",
    "        #recall@K\n",
    "        #rk = self.recallK(labels, scores,3)\n",
    "        #F1\n",
    "        scores[scores >= 0.5] = 1\n",
    "        scores[scores < 0.5] = 0\n",
    "        f1 = f1_score(y_true=labels, y_pred=scores)\n",
    "        \n",
    "        return auc, f1\n",
    "\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.n_users,self.test_data,self.weights)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self,args):\n",
    "        self.args=args\n",
    "        self.batch_size_cf = args.batch_size_cf\n",
    "        self.batch_size_kg = args.batch_size_kg\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        \n",
    "    #@pysnooper.snoop(prefix='ctr_eval')\n",
    "    def ctr_eval(self,sess, model, data,flag):\n",
    "        start = 0\n",
    "        auc_list = []\n",
    "        f1_list = []\n",
    "        rk_list = []\n",
    "        _batch = self.batch_size_cf//2\n",
    "        while start + _batch <= data.shape[0]:\n",
    "            auc, f1 = model.eval_(sess, self.get_test_feed_dict(model, data, start, start + _batch,flag))\n",
    "            auc_list.append(auc)\n",
    "            f1_list.append(f1)\n",
    "            \n",
    "            start += _batch\n",
    "        return float(np.mean(auc_list)), float(np.mean(f1_list))\n",
    "        \n",
    "    #@pysnooper.snoop(prefix='get_test_feed_dict')\n",
    "    def get_test_feed_dict(self,model, data, start, end,flag):\n",
    "        neg_items = []\n",
    "        users = data[start:end,0]\n",
    "        pos_items = data[start:end,1]\n",
    "        label = data[start:end,2]\n",
    "        \n",
    "        n_user=len(users)\n",
    "        all_user = np.squeeze(np.tile(users,(2,1)).reshape(-1,2*n_user))\n",
    "        \n",
    "        for u in users:\n",
    "            #pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += self.sample_neg_items_for_u(u, 1,flag)\n",
    "        all_item = np.squeeze(np.vstack((pos_items,neg_items)).reshape(-1,2*n_user))\n",
    "        all_label = np.pad(label,(0,n_user),'constant',constant_values=(0,0))\n",
    "        \n",
    "        neighbors = cdata.construct_neighbours(pos_items)\n",
    "        all_neighbors = np.vstack((neighbors,neighbors)).reshape(-1,self.args.neighbor_sample_size,2)\n",
    "        feed_dict = {\n",
    "            model.users: all_user,\n",
    "            model.pos_items: all_item,\n",
    "            model.neg_items: neg_items,\n",
    "            model.labels:all_label,\n",
    "            model.neighbors:all_neighbors,\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def sample_neg_items_for_u(self,u, num,flag):\n",
    "        neg_items = []\n",
    "        if(flag=='train'):\n",
    "            _user_dict = self.train_user_dict\n",
    "        if(flag=='eval'):\n",
    "            _user_dict = self.eval_user_dict\n",
    "        if(flag=='test'):\n",
    "            _user_dict = self.test_user_dict\n",
    "        while True:\n",
    "            if len(neg_items) == num: break\n",
    "            neg_i_id = np.random.randint(low=0, high=self.n_item,size=1)[0]\n",
    "\n",
    "            if neg_i_id not in _user_dict[u] and neg_i_id not in neg_items:\n",
    "                neg_items.append(neg_i_id)\n",
    "        return neg_items\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    def load_data(self):\n",
    "        self.n_user,self.n_item, self.train_data, self.eval_data, self.test_data,self.exist_users,self.train_user_dict,self.test_user_dict,self.eval_user_dict,self.n_interaction = self.load_rating()\n",
    "        #n_entity, n_relation, adj_entity, adj_relation = load_kg(args)\n",
    "        self.n_entity, self.n_relation, self.kg,self.rd,self.triplet_number = self.load_kg()\n",
    "        print('data loaded.')\n",
    "\n",
    "        return self.n_user, self.n_item, self.n_entity, self.n_relation, self.train_data, self.eval_data, self.test_data, self.kg,self.rd,self.n_interaction,self.triplet_number\n",
    "\n",
    "    #@pysnooper.snoop(prefix='load_kg')\n",
    "    def load_rating(self):\n",
    "        print('reading rating file ...')\n",
    "\n",
    "        # reading rating file\n",
    "        rating_file = './data/' + self.args.dataset + '/ratings_final'\n",
    "        if os.path.exists(rating_file + '.npy'):\n",
    "            rating_np = np.load(rating_file + '.npy')\n",
    "        else:\n",
    "            rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int64)\n",
    "            np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "        n_user = len(set(rating_np[:, 0]))\n",
    "        n_item = len(set(rating_np[:, 1]))\n",
    "        \n",
    "        n_interaction = len(rating_np)\n",
    "        \n",
    "        \n",
    "        train_data, eval_data, test_data = self.dataset_split(rating_np)\n",
    "\n",
    "        #exist_users=list(set(train_data[:,0]))\n",
    "        all_users=list(set(rating_np[:,0]))\n",
    "        \n",
    "        # {userid:[pos_id...]}}\n",
    "        train_user_dict=dict()\n",
    "        for id in range(len(train_data)):\n",
    "            u_id = train_data[id][0]\n",
    "            i_id = train_data[id][1]\n",
    "            if u_id not in train_user_dict.keys():\n",
    "                train_user_dict[u_id] = []\n",
    "            train_user_dict[u_id].append(i_id)\n",
    "        \n",
    "        test_user_dict=dict()\n",
    "        for id in range(len(test_data)):\n",
    "            u_id = test_data[id][0]\n",
    "            i_id = test_data[id][1]\n",
    "            if u_id not in test_user_dict.keys():\n",
    "                test_user_dict[u_id] = []\n",
    "            test_user_dict[u_id].append(i_id)        \n",
    "            \n",
    "        eval_user_dict=dict()\n",
    "        for id in range(len(eval_data)):\n",
    "            u_id = eval_data[id][0]\n",
    "            i_id = eval_data[id][1]\n",
    "            if u_id not in eval_user_dict.keys():\n",
    "                eval_user_dict[u_id] = []\n",
    "            eval_user_dict[u_id].append(i_id)        \n",
    "            \n",
    "        return n_user, n_item, train_data, eval_data, test_data,all_users,train_user_dict,test_user_dict,eval_user_dict,n_interaction\n",
    "\n",
    "\n",
    "    def dataset_split(self,rating_np):\n",
    "        print('splitting dataset ...')\n",
    "\n",
    "        # train:eval:test = 6:2:2\n",
    "        eval_ratio = 0.2\n",
    "        test_ratio = 0.2\n",
    "        n_ratings = rating_np.shape[0]\n",
    "\n",
    "        eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
    "        left = set(range(n_ratings)) - set(eval_indices)\n",
    "        test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "        train_indices = list(left - set(test_indices))\n",
    "        if self.args.ratio < 1:\n",
    "            train_indices = np.random.choice(list(train_indices), size=int(len(train_indices) * self.args.ratio), replace=False)\n",
    "\n",
    "        train_data = rating_np[train_indices]\n",
    "        eval_data = rating_np[eval_indices]\n",
    "        test_data = rating_np[test_indices]\n",
    "\n",
    "        \n",
    "        return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "    def load_kg(self):\n",
    "        print('reading KG file ...')\n",
    "\n",
    "        # reading kg file\n",
    "        kg_file = './data/' + self.args.dataset + '/kg_final'\n",
    "        if os.path.exists(kg_file + '.npy'):\n",
    "            kg_np = np.load(kg_file + '.npy')\n",
    "        else:\n",
    "            kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int64)\n",
    "            np.save(kg_file + '.npy', kg_np)\n",
    "\n",
    "        n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        n_relation = len(set(kg_np[:, 1]))\n",
    "        \n",
    "        triplet_number = len(kg_np)\n",
    "\n",
    "        kg,rd = self.construct_kg(kg_np)\n",
    "        #adj_entity, adj_relation = construct_adj(args, kg, n_entity)\n",
    "\n",
    "        return n_entity, n_relation, kg,rd ,triplet_number\n",
    "\n",
    "\n",
    "    def construct_kg(self,kg_np):\n",
    "        print('constructing knowledge graph ...')\n",
    "        kg = dict()\n",
    "        rd = dict()\n",
    "        for triple in kg_np:\n",
    "            head = triple[0]\n",
    "            relation = triple[1]\n",
    "            tail = triple[2]\n",
    "            # treat the KG as an undirected graph\n",
    "            if head not in kg.keys():\n",
    "                kg[head] = []\n",
    "            kg[head].append((tail, relation))\n",
    "            #if tail not in kg:\n",
    "            #    kg[tail] = []\n",
    "            #kg[tail].append((head, relation))\n",
    "            if relation not in rd.keys():\n",
    "                rd[relation]=[]\n",
    "            rd[relation].append((head,tail))\n",
    "        return kg,rd\n",
    "\n",
    "\n",
    "    def generate_train_feed_dict(self, model,start,end):\n",
    "        self.generate_cf_batch(start,end)\n",
    "        self.generate_kg_batch()\n",
    "        feed_dict = {\n",
    "            model.h: self.batch_data['heads'],\n",
    "            model.r: self.batch_data['relations'],\n",
    "            model.pos_t: self.batch_data['pos_tails'],\n",
    "            model.neg_t: self.batch_data['neg_tails'],\n",
    "\n",
    "            model.users: self.batch_data['users'],\n",
    "            model.pos_items: self.batch_data['pos_items'],\n",
    "            model.neg_items: self.batch_data['neg_items'],\n",
    "\n",
    "            model.neighbors: self.batch_data['neighbors'],\n",
    "            model.mess_dropout: eval(self.args.mess_dropout),\n",
    "            model.node_dropout: eval(self.args.node_dropout),\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "    \n",
    "   \n",
    "    def generate_cf_batch(self,start,end):\n",
    "        \n",
    "        users, pos_items, neg_items,neighbors = self._generate_train_cf_batch2(start,end)\n",
    "\n",
    "        self.batch_data = {}\n",
    "        self.batch_data['users'] = users\n",
    "        self.batch_data['pos_items'] = pos_items\n",
    "        self.batch_data['neg_items'] = neg_items\n",
    "        \n",
    "        self.batch_data['heads'] = pos_items\n",
    "        self.batch_data['neighbors'] = neighbors\n",
    "    \n",
    "    def generate_kg_batch(self):\n",
    "        heads, relations, pos_tails, neg_tails = self._generate_kg_batch()\n",
    "        self.batch_data['relations'] = relations\n",
    "        self.batch_data['pos_tails'] = pos_tails\n",
    "        self.batch_data['neg_tails'] = neg_tails\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _generate_train_cf_batch2(self,start,end):\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_item,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        users = self.train_data[start:end,0]\n",
    "        pos_items = self.train_data[start:end,1]\n",
    "        for u in users:\n",
    "            #pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        neighbors = cdata.construct_neighbours(pos_items)\n",
    "        return users, pos_items, neg_items,neighbors\n",
    "   \n",
    "    def _generate_kg_batch(self):\n",
    "        exist_heads = self.kg.keys()\n",
    "        '''\n",
    "        if self.batch_size_kg <= len(exist_heads):\n",
    "            heads = random.sample(exist_heads, self.batch_size_kg)\n",
    "        else:\n",
    "            heads = [random.choice(exist_heads) for _ in range(self.batch_size_kg)]\n",
    "        '''\n",
    "        heads = self.batch_data['pos_items']\n",
    "\n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.kg[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_entity, size=1)[0]\n",
    "                if (t, r) not in self.kg[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "\n",
    "        return heads, pos_r_batch, pos_t_batch, neg_t_batch\n",
    "        \n",
    "\n",
    "    def construct_neighbours(self,pos_items):\n",
    "        neighbor_sample_size=self.args.neighbor_sample_size\n",
    "        head_items = np.unique(pos_items)\n",
    "        end = np.zeros((len(pos_items),neighbor_sample_size,2))  # 2 (end relation)\n",
    "        for headid in head_items:\n",
    "            #print(headid)\n",
    "            #print(kg[userid])\n",
    "            elements = np.array(self.kg[headid])\n",
    "            numbers = len(elements)\n",
    "            if(numbers>=neighbor_sample_size):\n",
    "                sampled_indices = np.random.choice(list(range(numbers)),size=neighbor_sample_size,replace=False)\n",
    "            else:\n",
    "                sampled_indices = np.random.choice(list(range(numbers)),size=neighbor_sample_size,replace=True)\n",
    "            #print(sampled_indices)\n",
    "            #print(elements[sampled_indices,:])\n",
    "\n",
    "            idx = np.argwhere(pos_items == headid)\n",
    "            idx = np.array(idx.reshape(1, -1).squeeze(0))\n",
    "\n",
    "            end[idx,:] = elements[sampled_indices,:]\n",
    "        #print(end.astype(int))\n",
    "        return end.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "cdata = Data(args)\n",
    "data =cdata.load_data()\n",
    "# 0       1        2           3          4          5            6      7  8    9              10\n",
    "#n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg,rd,n_interaction,triplet_number\n",
    "\n",
    "#print(data)\n",
    "\n",
    "#print (data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:1872,item:3846,interactions:21173,entity:9366,relation:60,triplets:15518\n",
      "12705 4234 4234\n",
      "21173\n",
      "3846\n",
      "21173\n"
     ]
    }
   ],
   "source": [
    "print(\"user:%d,item:%d,interactions:%d,entity:%d,relation:%d,triplets:%d\"%(data[0],data[1],data[9],data[2],data[3],data[10]))\n",
    "print(data[4].shape[0],data[5].shape[0],data[6].shape[0])\n",
    "print(data[4].shape[0]+data[5].shape[0]+data[6].shape[0])\n",
    "print(len(data[7]))\n",
    "print(data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(0,100):\\n    if(len(k[i])>1):\\n        print(i,len(k[i]))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "k=data[7]\n",
    "#print(k.keys())\n",
    "#print(k[67])\n",
    "#print(k[14967])\n",
    "#print(len(k[14967]))\n",
    "#print(len(k[15061]))\n",
    "'''\n",
    "for i in range(0,100):\n",
    "    if(len(k[i])>1):\n",
    "        print(i,len(k[i]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, pos_items, neg_items,neighbors= cdata._generate_train_cf_batch2(1,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modelSource path:... <ipython-input-4-8f5002ba56da>\n",
      "modelStarting var:.. self = <__main__.RGAT object at 0x7f620c07a320>\n",
      "model07:35:07.162966 call       111     def fusion_neighbor(self):\n",
      "model07:35:07.163289 line       117         k = self.neighbors\n",
      "modelNew var:....... k = <tf.Tensor 'neighbours:0' shape=(?, 1, 2) dtype=int32>\n",
      "model07:35:07.163367 line       118         k_shape = self.neighbors.shape\n",
      "modelNew var:....... k_shape = TensorShape([Dimension(None), Dimension(1), Dimension(2)])\n",
      "model07:35:07.163965 line       119         shape = [self.batch_size_cf,1]\n",
      "modelNew var:....... shape = [2048, 1]\n",
      "model07:35:07.164105 line       122         self.neighbour_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neighbors[:,:,0])\n",
      "model07:35:07.176076 line       123         self.neighbour_r = tf.nn.embedding_lookup(self.weights['relation_embed'],self.neighbors[:,:,1])\n",
      "model07:35:07.183627 line       127         n_iter = 2\n",
      "modelNew var:....... n_iter = 2\n",
      "model07:35:07.183783 line       133         fusion = self.fussion_class(self.emb_dim,self.neighbor_sample_size)\n",
      "fusion_init    Source path:... <ipython-input-3-b5a1dd3cf572>\n",
      "fusion_init    Starting var:.. self = <__main__.SumFusion object at 0x7f6207c3a710>\n",
      "fusion_init    Starting var:.. emb_dim = 64\n",
      "fusion_init    Starting var:.. neighbor_sample_size = 1\n",
      "fusion_init    07:35:07.183963 call         5     def __init__(self,emb_dim,neighbor_sample_size):\n",
      "fusion_init    07:35:07.184212 line         6         self.neighbor_sample_size = neighbor_sample_size\n",
      "fusion_init    07:35:07.184290 line         7         self.emb_dim = emb_dim\n",
      "fusion_init    07:35:07.184360 line         8         x = self.neighbor_sample_size\n",
      "fusion_init    New var:....... x = 1\n",
      "fusion_init    07:35:07.184429 line         9         y = self.emb_dim\n",
      "fusion_init    New var:....... y = 64\n",
      "fusion_init    07:35:07.184524 return       9         y = self.emb_dim\n",
      "fusion_init    Return value:.. None\n",
      "fusion_init    Elapsed time: 00:00:00.000712\n",
      "modelNew var:....... fusion = <__main__.SumFusion object at 0x7f6207c3a710>\n",
      "model07:35:07.258364 line       136         new_embedding = fusion(\n",
      "model07:35:07.258706 line       137                                 neighbor_entities=self.neighbour_e,\n",
      "model07:35:07.258934 line       138                                 neighbor_relations=self.neighbour_r,\n",
      "model07:35:07.259137 line       139                                 user_embeddings=self.u_e)\n",
      "sum    Source path:... <ipython-input-3-b5a1dd3cf572>\n",
      "sum    Starting var:.. self = <__main__.SumFusion object at 0x7f6207c3a710>\n",
      "sum    Starting var:.. neighbor_entities = <tf.Tensor 'embedding_lookup_3/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "sum    Starting var:.. neighbor_relations = <tf.Tensor 'embedding_lookup_4/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "sum    Starting var:.. user_embeddings = <tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 64) dtype=float32>\n",
      "sum    07:35:07.259379 call        80     def fusion(self, neighbor_entities, neighbor_relations, user_embeddings):\n",
      "sum    07:35:07.260682 line        82         neighbors_fusion = self._fusion_neighbor_entities(neighbor_entities, neighbor_relations)\n",
      "mix        Source path:... <ipython-input-3-b5a1dd3cf572>\n",
      "mix        Starting var:.. self = <__main__.SumFusion object at 0x7f6207c3a710>\n",
      "mix        Starting var:.. neighbor_entities = <tf.Tensor 'embedding_lookup_3/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        Starting var:.. neighbor_relations = <tf.Tensor 'embedding_lookup_4/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        07:35:07.260942 call        13     def _fusion_neighbor_entities(self, neighbor_entities, neighbor_relations):\n",
      "mix        07:35:07.261286 line        30         _relation_scores = tf.matmul(tf.multiply(neighbor_relations,neighbor_entities), self.weights) + self.bias\n",
      "mix        New var:....... _relation_scores = <tf.Tensor 'add:0' shape=(?, 1, 1) dtype=float32>\n",
      "mix        07:35:07.269325 line        31         _relation_scores = tf.squeeze(_relation_scores, axis=2)\n",
      "mix        Modified var:.. _relation_scores = <tf.Tensor 'Squeeze:0' shape=(?, 1) dtype=float32>\n",
      "mix        07:35:07.272311 line        33         _relation_scores_normalized = tf.nn.softmax(_relation_scores, dim=-1)\n",
      "mix        New var:....... _relation_scores_normalized = <tf.Tensor 'Softmax:0' shape=(?, 1) dtype=float32>\n",
      "mix        07:35:07.276573 line        36         _relation_scores_normalized = tf.expand_dims(_relation_scores_normalized, axis=-1)\n",
      "mix        Modified var:.. _relation_scores_normalized = <tf.Tensor 'ExpandDims:0' shape=(?, 1, 1) dtype=float32>\n",
      "mix        07:35:07.281829 line        41         neighbors_fusion = _relation_scores_normalized * neighbor_entities\n",
      "mix        New var:....... neighbors_fusion = <tf.Tensor 'mul_1:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        07:35:07.284504 line        43         conv1D = tf.keras.layers.Conv1D(self.emb_dim,self.neighbor_sample_size,self.neighbor_sample_size,padding='valid')\n",
      "mix        New var:....... conv1D = <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7f6207bd9d68>\n",
      "mix        07:35:07.376655 line        44         neighbors_fusion = conv1D(neighbors_fusion)\n",
      "mix        Modified var:.. neighbors_fusion = <tf.Tensor 'conv1d/BiasAdd:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        07:35:07.499777 line        48         neighbors_fusion = tf.squeeze(neighbors_fusion,axis=1)\n",
      "mix        Modified var:.. neighbors_fusion = <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "mix        07:35:07.502787 line        53         return neighbors_fusion\n",
      "mix        07:35:07.503252 return      53         return neighbors_fusion\n",
      "mix        Return value:.. <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "mix        Elapsed time: 00:00:00.242778\n",
      "sum    New var:....... neighbors_fusion = <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "sum    07:35:07.503832 line        86         output = tf.concat([user_embeddings, neighbors_fusion], axis=-1)\n",
      "sum    New var:....... output = <tf.Tensor 'concat:0' shape=(?, 128) dtype=float32>\n",
      "sum    07:35:07.508714 line        88         output = tf.matmul(output, self.weights2) + self.bias2\n",
      "sum    Modified var:.. output = <tf.Tensor 'add_1:0' shape=(?, 64) dtype=float32>\n",
      "sum    07:35:07.514385 line        93         return self.act(output)   \n",
      "sum    07:35:07.516848 return      93         return self.act(output)   \n",
      "sum    Return value:.. <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "sum    Elapsed time: 00:00:00.258003\n",
      "modelNew var:....... new_embedding = <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "model07:35:07.517497 line       144         return new_embedding #, aggregators        \n",
      "model07:35:07.517823 return     144         return new_embedding #, aggregators        \n",
      "modelReturn value:.. <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "modelElapsed time: 00:00:00.355217\n"
     ]
    }
   ],
   "source": [
    "model = RGAT(data,args,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'   #指定第一块GPU可用\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 程序最多只能占用指定gpu50%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [3.1s]: [all_loss:4.85197, cf_loss:4.85199, cf_reg_loss:0.00000, kge_loss:4.85188, kge_reg_loss 0.00003 ]\n",
      "---epoch 0    train auc: 0.7021  f1: 0.6673    eval auc: 0.6865  f1: 0.6502    test auc: 0.6783  f1: 0.6477\n",
      "Epoch 1 [1.6s]: [all_loss:4.85160, cf_loss:4.85179, cf_reg_loss:0.00000, kge_loss:4.85088, kge_reg_loss 0.00003 ]\n",
      "---epoch 1    train auc: 0.8076  f1: 0.7341    eval auc: 0.7768  f1: 0.7083    test auc: 0.7673  f1: 0.7023\n",
      "Epoch 2 [1.5s]: [all_loss:4.85064, cf_loss:4.85128, cf_reg_loss:0.00000, kge_loss:4.84809, kge_reg_loss 0.00004 ]\n",
      "---epoch 2    train auc: 0.8448  f1: 0.7663    eval auc: 0.8018  f1: 0.7265    test auc: 0.7958  f1: 0.7208\n",
      "Epoch 3 [1.5s]: [all_loss:4.84846, cf_loss:4.84998, cf_reg_loss:0.00001, kge_loss:4.84235, kge_reg_loss 0.00004 ]\n",
      "---epoch 3    train auc: 0.8534  f1: 0.7783    eval auc: 0.8103  f1: 0.7405    test auc: 0.7992  f1: 0.7338\n",
      "Epoch 4 [1.5s]: [all_loss:4.84371, cf_loss:4.84663, cf_reg_loss:0.00003, kge_loss:4.83206, kge_reg_loss 0.00004 ]\n",
      "---epoch 4    train auc: 0.8528  f1: 0.7743    eval auc: 0.8129  f1: 0.7470    test auc: 0.8006  f1: 0.7358\n",
      "Epoch 5 [1.5s]: [all_loss:4.83452, cf_loss:4.83884, cf_reg_loss:0.00008, kge_loss:4.81723, kge_reg_loss 0.00005 ]\n",
      "---epoch 5    train auc: 0.8509  f1: 0.7712    eval auc: 0.8187  f1: 0.7509    test auc: 0.8085  f1: 0.7398\n",
      "Epoch 6 [1.6s]: [all_loss:4.81650, cf_loss:4.82144, cf_reg_loss:0.00026, kge_loss:4.79674, kge_reg_loss 0.00006 ]\n",
      "---epoch 6    train auc: 0.8544  f1: 0.7737    eval auc: 0.8216  f1: 0.7534    test auc: 0.8069  f1: 0.7392\n",
      "Epoch 7 [1.5s]: [all_loss:4.78376, cf_loss:4.78710, cf_reg_loss:0.00073, kge_loss:4.77038, kge_reg_loss 0.00008 ]\n",
      "---epoch 7    train auc: 0.8556  f1: 0.7709    eval auc: 0.8272  f1: 0.7534    test auc: 0.8115  f1: 0.7384\n",
      "Epoch 8 [1.5s]: [all_loss:4.72202, cf_loss:4.71909, cf_reg_loss:0.00199, kge_loss:4.73374, kge_reg_loss 0.00010 ]\n",
      "---epoch 8    train auc: 0.8566  f1: 0.7691    eval auc: 0.8250  f1: 0.7461    test auc: 0.8163  f1: 0.7385\n",
      "Epoch 10 [1.5s]: [all_loss:4.45539, cf_loss:4.41957, cf_reg_loss:0.01144, kge_loss:4.59870, kge_reg_loss 0.00018 ]\n",
      "---epoch 10    train auc: 0.8596  f1: 0.7649    eval auc: 0.8332  f1: 0.7460    test auc: 0.8222  f1: 0.7355\n",
      "Epoch 11 [1.5s]: [all_loss:4.21629, cf_loss:4.15159, cf_reg_loss:0.02440, kge_loss:4.47508, kge_reg_loss 0.00025 ]\n",
      "---epoch 11    train auc: 0.8625  f1: 0.7709    eval auc: 0.8291  f1: 0.7451    test auc: 0.8285  f1: 0.7408\n",
      "Epoch 12 [1.5s]: [all_loss:3.91284, cf_loss:3.81471, cf_reg_loss:0.04843, kge_loss:4.30538, kge_reg_loss 0.00035 ]\n",
      "---epoch 12    train auc: 0.8593  f1: 0.7705    eval auc: 0.8292  f1: 0.7489    test auc: 0.8218  f1: 0.7447\n",
      "Epoch 13 [1.6s]: [all_loss:3.57127, cf_loss:3.44829, cf_reg_loss:0.08900, kge_loss:4.06321, kge_reg_loss 0.00050 ]\n",
      "---epoch 13    train auc: 0.8589  f1: 0.7768    eval auc: 0.8268  f1: 0.7517    test auc: 0.8202  f1: 0.7502\n",
      "Epoch 14 [1.6s]: [all_loss:3.28123, cf_loss:3.15824, cf_reg_loss:0.15017, kge_loss:3.77321, kge_reg_loss 0.00072 ]\n",
      "---epoch 14    train auc: 0.8572  f1: 0.7803    eval auc: 0.8277  f1: 0.7584    test auc: 0.8260  f1: 0.7594\n",
      "Epoch 15 [1.5s]: [all_loss:3.01091, cf_loss:2.89763, cf_reg_loss:0.22903, kge_loss:3.46402, kge_reg_loss 0.00100 ]\n",
      "---epoch 15    train auc: 0.8566  f1: 0.7836    eval auc: 0.8245  f1: 0.7597    test auc: 0.8208  f1: 0.7616\n",
      "Epoch 16 [1.6s]: [all_loss:2.83400, cf_loss:2.75410, cf_reg_loss:0.30361, kge_loss:3.15359, kge_reg_loss 0.00134 ]\n",
      "---epoch 16    train auc: 0.8540  f1: 0.7848    eval auc: 0.8345  f1: 0.7708    test auc: 0.8106  f1: 0.7577\n",
      "Epoch 17 [1.6s]: [all_loss:2.70962, cf_loss:2.67435, cf_reg_loss:0.36367, kge_loss:2.85070, kge_reg_loss 0.00173 ]\n",
      "---epoch 17    train auc: 0.8527  f1: 0.7868    eval auc: 0.8263  f1: 0.7691    test auc: 0.8191  f1: 0.7660\n",
      "Epoch 18 [1.6s]: [all_loss:2.60276, cf_loss:2.60563, cf_reg_loss:0.38271, kge_loss:2.59127, kge_reg_loss 0.00215 ]\n",
      "---epoch 18    train auc: 0.8547  f1: 0.7907    eval auc: 0.8208  f1: 0.7663    test auc: 0.8222  f1: 0.7681\n",
      "Epoch 20 [1.5s]: [all_loss:2.45229, cf_loss:2.51563, cf_reg_loss:0.36529, kge_loss:2.19892, kge_reg_loss 0.00303 ]\n",
      "---epoch 20    train auc: 0.8549  f1: 0.7925    eval auc: 0.8272  f1: 0.7765    test auc: 0.8210  f1: 0.7735\n",
      "Epoch 21 [1.7s]: [all_loss:2.39190, cf_loss:2.46530, cf_reg_loss:0.34979, kge_loss:2.09826, kge_reg_loss 0.00345 ]\n",
      "---epoch 21    train auc: 0.8557  f1: 0.7951    eval auc: 0.8281  f1: 0.7783    test auc: 0.8171  f1: 0.7716\n",
      "Epoch 22 [1.6s]: [all_loss:2.35411, cf_loss:2.45669, cf_reg_loss:0.33318, kge_loss:1.94376, kge_reg_loss 0.00383 ]\n",
      "---epoch 22    train auc: 0.8518  f1: 0.7938    eval auc: 0.8255  f1: 0.7771    test auc: 0.8224  f1: 0.7751\n",
      "Epoch 23 [1.5s]: [all_loss:2.30784, cf_loss:2.40806, cf_reg_loss:0.31794, kge_loss:1.90695, kge_reg_loss 0.00421 ]\n",
      "---epoch 23    train auc: 0.8540  f1: 0.7970    eval auc: 0.8259  f1: 0.7807    test auc: 0.8191  f1: 0.7738\n",
      "Epoch 24 [1.5s]: [all_loss:2.25860, cf_loss:2.36769, cf_reg_loss:0.31005, kge_loss:1.82223, kge_reg_loss 0.00456 ]\n",
      "---epoch 24    train auc: 0.8534  f1: 0.7959    eval auc: 0.8258  f1: 0.7807    test auc: 0.8158  f1: 0.7732\n",
      "Epoch 25 [1.5s]: [all_loss:2.23095, cf_loss:2.35477, cf_reg_loss:0.30757, kge_loss:1.73568, kge_reg_loss 0.00487 ]\n",
      "---epoch 25    train auc: 0.8564  f1: 0.8001    eval auc: 0.8238  f1: 0.7805    test auc: 0.8170  f1: 0.7742\n",
      "Epoch 26 [1.5s]: [all_loss:2.23130, cf_loss:2.36696, cf_reg_loss:0.30416, kge_loss:1.68866, kge_reg_loss 0.00517 ]\n",
      "---epoch 26    train auc: 0.8501  f1: 0.7967    eval auc: 0.8282  f1: 0.7844    test auc: 0.8188  f1: 0.7785\n",
      "Epoch 27 [1.6s]: [all_loss:2.16185, cf_loss:2.29095, cf_reg_loss:0.30332, kge_loss:1.64549, kge_reg_loss 0.00545 ]\n",
      "---epoch 27    train auc: 0.8513  f1: 0.7981    eval auc: 0.8190  f1: 0.7793    test auc: 0.8198  f1: 0.7776\n",
      "Epoch 28 [1.5s]: [all_loss:2.16194, cf_loss:2.31269, cf_reg_loss:0.30141, kge_loss:1.55896, kge_reg_loss 0.00572 ]\n",
      "---epoch 28    train auc: 0.8553  f1: 0.8012    eval auc: 0.8254  f1: 0.7854    test auc: 0.8114  f1: 0.7757\n"
     ]
    }
   ],
   "source": [
    "train_data = data[4]\n",
    "eval_data = data[5]\n",
    "test_data = data[6]\n",
    "for epoch in range(args.n_epochs):\n",
    "    t1 = time()\n",
    "    loss, base_loss,cf_loss,cf_reg_loss, kge_loss, kge_reg_loss = 0., 0., 0., 0.,0.,0.\n",
    "    #n_batch = cdata.n_train // args.batch_size_cf + 1\n",
    "\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Alternative Training for KGAT:\n",
    "    ... phase 1: to train the recommender.\n",
    "    \n",
    "    for idx in range(n_batch):\n",
    "        btime= time()\n",
    "\n",
    "        batch_data = data_generator.generate_train_batch()\n",
    "        feed_dict = data_generator.generate_train_feed_dict(model, batch_data)\n",
    "\n",
    "        _, batch_loss, batch_base_loss, batch_kge_loss, batch_reg_loss = model.train(sess, feed_dict=feed_dict)\n",
    "\n",
    "        loss += batch_loss\n",
    "        base_loss += batch_base_loss\n",
    "        kge_loss += batch_kge_loss\n",
    "        reg_loss += batch_reg_loss\n",
    "\n",
    "    if np.isnan(loss) == True:\n",
    "        print('ERROR: loss@phase1 is nan.')\n",
    "        sys.exit()\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    *********************************************************\n",
    "    Alternative Training for KGAT:\n",
    "    ... phase 2: to train the KGE method & update the attentive Laplacian matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    n_batch = len(train_data) // args.batch_size_cf + 1\n",
    "\n",
    "    if args.use_kge is True:\n",
    "        # using KGE method (knowledge graph embedding).\n",
    "        batch_kg_loss =0.0\n",
    "        batch_kg_reg_loss = 0.0\n",
    "        for idx in range(n_batch):\n",
    "            start = idx*args.batch_size_cf\n",
    "            end = start+args.batch_size_cf\n",
    "            feed_dict = cdata.generate_train_feed_dict(model,start,end) #train\n",
    "            start = end\n",
    "            #_,_, batch_loss,batch_cf_loss,batch_kg_loss,batch_kg_reg_loss = model.train(sess,feed_dict = feed_dict)\n",
    "            _, batch_loss,batch_cf_loss,batch_cf_reg_loss,batch_kg_loss,batch_kg_reg_loss = model.train(sess,feed_dict = feed_dict)\n",
    "            \n",
    "            loss += batch_loss\n",
    "            kge_loss += batch_kg_loss\n",
    "            cf_loss += batch_cf_loss\n",
    "            cf_reg_loss +=batch_cf_reg_loss\n",
    "            kge_reg_loss += batch_kg_reg_loss\n",
    "\n",
    "    #if args.use_att is True:\n",
    "        # updating attentive laplacian matrix.\n",
    "    #    model.update_attentive_A(sess)\n",
    "\n",
    "    if np.isnan(loss) == True:\n",
    "        print('ERROR: loss@phase2 is nan.')\n",
    "        sys.exit()\n",
    "\n",
    "    show_step = 10\n",
    "    \n",
    "    if (epoch + 1) % show_step != 0:\n",
    "        if args.verbose > 0 and epoch % args.verbose == 0:\n",
    "            \n",
    "            perf_str = 'Epoch %d [%.1fs]: [all_loss:%.5f, cf_loss:%.5f, cf_reg_loss:%.5f, kge_loss:%.5f, kge_reg_loss %.5f ]' % (epoch, time() - t1,loss,cf_loss,cf_reg_loss, kge_loss,kge_reg_loss)\n",
    "            print(perf_str)\n",
    "            train_auc, train_f1 = cdata.ctr_eval(sess, model, train_data,'train')\n",
    "            eval_auc, eval_f1 = cdata.ctr_eval(sess, model, eval_data,'eval')\n",
    "            test_auc, test_f1 = cdata.ctr_eval(sess, model, test_data,'test')\n",
    "            \n",
    "            print('---epoch %d    train auc: %.4f  f1: %.4f    eval auc: %.4f  f1: %.4f    test auc: %.4f  f1: %.4f'\n",
    "                  % (epoch, train_auc, train_f1, eval_auc, eval_f1, test_auc, test_f1))\n",
    "            #print('---epoch %d    train auc: %.4f  f1: %.4f '\n",
    "            #      % (epoch, train_auc, train_f1))\n",
    "        continue\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
