{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from time import time\n",
    "import numpy as np\n",
    "import random \n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import heapq\n",
    "import pysnooper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aggregator='cnn', batch_size_cf=2048, batch_size_kg=2048, dataset='music', dropout=0.1, emb_dim=64, kge_dim=64, l2_weight=1e-07, lr=0.0005, mess_dropout='[0.1]', model_path='', n_epochs=50, n_iter=2, neighbor_sample_size=1, node_dropout='[0.1]', ratio=1, regs='[1e-5,1e-5,1e-2]', seed=111, use_att=True, use_kge=True, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# movie\n",
    "parser.add_argument('--model_path', nargs='?', default='',help='Store model path.')\n",
    "parser.add_argument('--dataset', type=str, default='music', help='which dataset to use')\n",
    "parser.add_argument('--aggregator', type=str, default='cnn', help='which aggregator to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=50, help='the number of epochs')\n",
    "parser.add_argument('--neighbor_sample_size', type=int, default=1, help='the number of neighbors to be sampled')\n",
    "parser.add_argument('--emb_dim', type=int, default=64, help='dimension of user and entity embeddings')\n",
    "parser.add_argument('--kge_dim', type=int, default=64, help='dimension of user and KG embeddings')\n",
    "parser.add_argument('--n_iter', type=int, default=2, help='number of iterations when computing entity representation')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-7, help='weight of l2 regularization')\n",
    "parser.add_argument('--lr', type=float, default=0.0005, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=1, help='size of training dataset')\n",
    "parser.add_argument('--seed', type=int, default=111, help='random seed')\n",
    "parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',help='Regularization for user and item embeddings.')\n",
    "\n",
    "parser.add_argument('--batch_size_cf', type=int, default=2048,help='CF batch size.')\n",
    "parser.add_argument('--batch_size_kg', type=int, default=2048,help='KG batch size.')\n",
    "\n",
    "parser.add_argument('--use_kge', type=bool, default=True,help='whether using knowledge graph embedding')\n",
    "parser.add_argument('--use_att', type=bool, default=True,help='whether using attention mechanism')   \n",
    "parser.add_argument('--verbose', type=int, default=1,help='Interval of evaluation.')\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='keep probability(1-dropout)')\n",
    "parser.add_argument('--mess_dropout', nargs='?', default='[0.1]',\n",
    "                      help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "parser.add_argument('--node_dropout', nargs='?', default='[0.1]',\n",
    "                      help='Keep probability w.r.t. node dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "tf.set_random_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "class Fusion(object):\n",
    "    @pysnooper.snoop(prefix='fusion_init')\n",
    "    def __init__(self,emb_dim,neighbor_sample_size):\n",
    "        self.neighbor_sample_size = neighbor_sample_size\n",
    "        self.emb_dim = emb_dim\n",
    "        x = self.neighbor_sample_size\n",
    "        y = self.emb_dim\n",
    "\n",
    "    \n",
    "    @pysnooper.snoop(prefix='mix')\n",
    "    def _fusion_neighbor_entities(self, neighbor_entities, neighbor_relations):\n",
    "       \n",
    "        #pad_numb = self.n_relation - len(neighbor_relations)\n",
    "        #neighbor_relations = np.pad(neighbor_relations,(0,3),'constant',constant_values(0,0))\n",
    "        # 第一个维度(行）前面扩展0行，后面扩展3行，第二个维度（列)不变\n",
    "        #d = np.pad(d,((0,3),(0,0)),'constant',constant_values=(0,0))\n",
    "\n",
    "\n",
    "        #  [batch_size,  n_neighbor, dim] * [batch_size,  n_neighbor, dim]--> [batch_size, n_neighbor,1]---\n",
    "        #user_relation_scores = tf.reduce_mean(neighbor_vectors * neighbor_relations, axis=-1)---\n",
    "        \n",
    "        #[batch_size,neighbor_sample_size,dim]--->[batch_size,neighbor_sample_size]\n",
    "        #_relation_scores = tf.reduce_mean(neighbor_relations, axis=-1)\n",
    "        _relation_scores = tf.matmul(tf.multiply(neighbor_relations,neighbor_entities), self.weights) + self.bias\n",
    "        _relation_scores = tf.squeeze(_relation_scores, axis=2)\n",
    "         # [batch_size,  neighbor_sample_size] --> [batch_size, neighbor_sample_size]\n",
    "        _relation_scores_normalized = tf.nn.softmax(_relation_scores, dim=-1)\n",
    "\n",
    "        # [batch_size, neighbor_sample_size] --> [batch_size, neighbor_sample_size, 1]\n",
    "        _relation_scores_normalized = tf.expand_dims(_relation_scores_normalized, axis=-1)\n",
    "\n",
    "        # 1.[batch_size, neighbor_sample_size, 1]   [batch_size,, neighbor_sample_size, dim]--> [batch_size, neighbor_sample_size, dim]\n",
    "        # 2.[batch_size, neighbor_sample_size, dim] ---> [batch_size, dim]\n",
    "        #neighbors_fusion = tf.reduce_mean(_relation_scores_normalized * neighbor_entities, axis=1)\n",
    "        neighbors_fusion = _relation_scores_normalized * neighbor_entities\n",
    "\n",
    "        conv1D = tf.keras.layers.Conv1D(self.emb_dim,self.neighbor_sample_size,self.neighbor_sample_size,padding='valid')\n",
    "        neighbors_fusion = conv1D(neighbors_fusion)\n",
    "        #conv1d = tf.keras.layers.Conv1D(1,1,1,padding='valid')\n",
    "        #max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=10,strides=10,padding='valid')\n",
    "        #neighbors_fusion = max_pool_1d(neighbors_fusion)\n",
    "        neighbors_fusion = tf.squeeze(neighbors_fusion,axis=1)\n",
    "        #neighbors_fusion = tf.nn.softmax(tf.reduce_sum(_relation_scores_normalized * neighbor_entities, axis=2),dim=1)\n",
    "        #neighbors_fusion = tf.nn.softmax(_relation_scores_normalized * neighbor_entities,dim=1)\n",
    "        #neighbors_fusion = tf.reduce_max(_relation_scores_normalized * neighbor_entities, axis=1)\n",
    "        \n",
    "        return neighbors_fusion\n",
    "\n",
    "    @pysnooper.snoop(prefix='mix')\n",
    "    def _cnn_fusion_neighbor_entities(self, neighbor_entities, neighbor_relations):\n",
    "       \n",
    "        E_R = tf.matmul(tf.transpose(neighbor_relations, perm=[0,2,1]),neighbor_entities)\n",
    "        E_R = tf.expand_dims(E_R, -1)\n",
    "        conv2D_1 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
    "        \n",
    "        E_R = conv2D_1(E_R)\n",
    "        conv2D_2 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
    "        E_R = conv2D_2(E_R)\n",
    "        conv2D_3 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
    "        E_R = conv2D_3(E_R)\n",
    "\n",
    "        conv2D_4 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
    "        E_R = conv2D_4(E_R)\n",
    "        \n",
    "        conv2D_5 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
    "        E_R = conv2D_5(E_R)\n",
    "        \n",
    "        conv2D_6 = tf.keras.layers.Conv2D(1,4,strides=(2,2),padding='same',data_format=None)\n",
    "        E_R = conv2D_6(E_R)\n",
    "        \n",
    "        _relation_scores = tf.squeeze(E_R, axis=2)\n",
    "        _relation_scores_normalized = tf.nn.softmax(_relation_scores, dim=-1)\n",
    "        neighbors_fusion = _relation_scores_normalized * neighbor_entities\n",
    "\n",
    "        conv1D = tf.keras.layers.Conv1D(self.emb_dim,self.neighbor_sample_size,self.neighbor_sample_size,padding='valid')\n",
    "        neighbors_fusion = conv1D(neighbors_fusion)\n",
    "\n",
    "        neighbors_fusion = tf.squeeze(neighbors_fusion,axis=1)\n",
    "\n",
    "        return neighbors_fusion    \n",
    "\n",
    "    def __call__(self,neighbor_entities, neighbor_relations,user_embeddings):\n",
    "        new_embedding = self.fusion(neighbor_entities, neighbor_relations, user_embeddings)\n",
    "        return new_embedding\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fusion(self,neighbor_entities, neighbor_relations,user_embeddings):\n",
    "        # dimension:\n",
    "        # neighbor_vectors: [batch_size, neighbor_sample_size, n_neighbor, dim]\n",
    "        # neighbor_relations: [batch_size, -neighbor_sample_size, dim]\n",
    "        pass\n",
    "\n",
    "\n",
    "class CNNFusion(Fusion):\n",
    "    def __init__(self, emb_dim,neighbor_sample_size,dropout=0.1, act=tf.nn.relu):\n",
    "\n",
    "        super(CNNFusion, self).__init__(emb_dim,neighbor_sample_size)\n",
    "        self.act = act\n",
    "        self.dropout=dropout\n",
    "        '''\n",
    "        initializer = tf.keras.initializers.glorot_normal()\n",
    "        self.weights = tf.get_variable(shape=[self.emb_dim, 1], initializer=tf.keras.initializers.glorot_normal(),name='weights1')\n",
    "        self.bias = tf.get_variable(shape=[1], initializer=tf.zeros_initializer(), name='bias1')\n",
    "        \n",
    "        self.weights2 = tf.get_variable(shape=[self.emb_dim*2, self.emb_dim], initializer=tf.keras.initializers.glorot_normal(),name='weights2')\n",
    "        self.bias2 = tf.get_variable(shape=[self.emb_dim], initializer=tf.zeros_initializer(), name='bias2')\n",
    "        '''\n",
    "    @pysnooper.snoop(prefix='cnn')\n",
    "    def fusion(self, neighbor_entities, neighbor_relations, user_embeddings):\n",
    "        neighbors_fusion = self._cnn_fusion_neighbor_entities(neighbor_entities, neighbor_relations)\n",
    "        '''\n",
    "        output = tf.concat([user_embeddings, neighbors_fusion], axis=-1)\n",
    "        output = tf.matmul(output, self.weights2) + self.bias2\n",
    "        '''\n",
    "        return self.act(neighbors_fusion)       \n",
    "    \n",
    "class ConcatFusion(Fusion):\n",
    "    def __init__(self, emb_dim,neighbor_sample_size,dropout=0.1, act=tf.nn.relu):\n",
    "        #self.dropout = dropout\n",
    "        super(ConcatFusion, self).__init__(emb_dim,neighbor_sample_size)\n",
    "        self.act = act\n",
    "        self.dropout=dropout\n",
    "        \n",
    "        initializer = tf.keras.initializers.glorot_normal()\n",
    "        self.weights = tf.get_variable(shape=[self.emb_dim, 1], initializer=tf.keras.initializers.glorot_normal(),name='weights1')\n",
    "        self.bias = tf.get_variable(shape=[1], initializer=tf.zeros_initializer(), name='bias1')\n",
    "        \n",
    "        self.weights2 = tf.get_variable(shape=[self.emb_dim*2, self.emb_dim], initializer=tf.keras.initializers.glorot_normal(),name='weights2')\n",
    "        self.bias2 = tf.get_variable(shape=[self.emb_dim], initializer=tf.zeros_initializer(), name='bias2')\n",
    "    @pysnooper.snoop(prefix='concat')\n",
    "    def fusion(self, neighbor_entities, neighbor_relations, user_embeddings):\n",
    "        \n",
    "        neighbors_fusion = self._fusion_neighbor_entities(neighbor_entities, neighbor_relations)\n",
    "\n",
    "        #output = user_embeddings + neighbors_fusion\n",
    "        \n",
    "        output = tf.concat([user_embeddings, neighbors_fusion], axis=-1)\n",
    "        #output = tf.nn.dropout(output, keep_prob=1-self.dropout)\n",
    "        output = tf.matmul(output, self.weights2) + self.bias2\n",
    "        \n",
    "        # [batch_size, dim]\n",
    "        #output = tf.reshape(output, [self.batch_size, self.dim])\n",
    "\n",
    "        return self.act(output)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "#@pysnooper.snoop(prefix='RGAT')\n",
    "class RGAT(object):\n",
    "    def __init__(self,data,args,pretrain_data):\n",
    "        self._parse_args(data, args,pretrain_data)\n",
    "        self._build_inputs()\n",
    "        self.weights = self._build_weights()\n",
    "        self._build_model_cf()\n",
    "        self._build_model_kg()\n",
    "        self._build_loss()\n",
    "        \n",
    "        self.items = data[1]\n",
    "        \n",
    "    def _parse_args(self,data, args,pretrain_data):\n",
    "        #n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg,rd\n",
    "        self.n_users = data[0]\n",
    "        self.n_items = data[1]\n",
    "        self.n_entities = data[2]\n",
    "        self.n_relations = data[3]\n",
    "\n",
    "        self.dropout=args.dropout\n",
    "        self.kge_dim = args.kge_dim\n",
    "        self.emb_dim = args.emb_dim\n",
    "        \n",
    "        self.batch_size_kg = args.batch_size_kg\n",
    "        self.batch_size_cf = args.batch_size_cf\n",
    "        \n",
    "        self.regs = eval(args.regs)\n",
    "        self.lr =args.lr\n",
    "        \n",
    "        self.pretrain_data = pretrain_data\n",
    "        self.neighbor_sample_size = args.neighbor_sample_size\n",
    "        self.fussion_class = CNNFusion #ConcatFusion\n",
    "\n",
    "        \n",
    "    def _build_inputs(self):\n",
    "        # placeholder definition\n",
    "        self.users = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.pos_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "        self.neg_items = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # for knowledge graph modeling (TransD)\n",
    "        #self.A_values = tf.placeholder(tf.float32, shape=[len(self.all_v_list)], name='A_values')\n",
    "\n",
    "        self.h = tf.placeholder(tf.int32, shape=[None], name='h')\n",
    "        self.r = tf.placeholder(tf.int32, shape=[None], name='r')\n",
    "        self.pos_t = tf.placeholder(tf.int32, shape=[None], name='pos_t')\n",
    "        self.neg_t = tf.placeholder(tf.int32, shape=[None], name='neg_t')\n",
    "        # 2 --> (end,relation)\n",
    "        self.neighbors = tf.placeholder(tf.int32,shape=[None,self.neighbor_sample_size,2],name ='neighbours')\n",
    "\n",
    "        self.node_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.mess_dropout = tf.placeholder(tf.float32, shape=[None])\n",
    "    \n",
    "        self.labels = tf.placeholder(dtype=tf.float32, shape=[None], name='labels')\n",
    "        \n",
    "    def _build_weights(self):\n",
    "        all_weights = dict()\n",
    "        initializer = tf.keras.initializers.glorot_normal()\n",
    "\n",
    "        if self.pretrain_data is None:\n",
    "            all_weights['user_embed'] = tf.Variable(initializer([self.n_users, self.emb_dim]), name='user_embed')\n",
    "            all_weights['entity_embed'] = tf.Variable(initializer([self.n_entities, self.emb_dim]), name='entity_embed')\n",
    "            print('using xavier initialization')\n",
    "        else:\n",
    "            all_weights['user_embed'] = tf.Variable(initial_value=self.pretrain_data['user_embed'], trainable=True,\n",
    "                                                    name='user_embed', dtype=tf.float32)\n",
    "\n",
    "            item_embed = self.pretrain_data['item_embed']\n",
    "            other_embed = initializer([self.n_entities - self.n_items, self.emb_dim])\n",
    "\n",
    "            all_weights['entity_embed'] = tf.Variable(initial_value=tf.concat([item_embed, other_embed], 0),\n",
    "                                                      trainable=True, name='entity_embed', dtype=tf.float32)\n",
    "            print('using pretrained initialization')\n",
    "\n",
    "        all_weights['relation_embed'] = tf.Variable(initializer([self.n_relations, self.kge_dim]),\n",
    "                                                    name='relation_embed')\n",
    "        all_weights['trans_W'] = tf.Variable(initializer([self.n_relations, self.emb_dim, self.kge_dim]))\n",
    "        all_weights['trans_U'] = tf.Variable(initializer([self.emb_dim,self.emb_dim]))\n",
    "        \n",
    "        self.cf_weights=tf.Variable(initializer([self.kge_dim*2,1]),name='cf_weights')\n",
    "        self.cf_bias = tf.get_variable(shape=[1], initializer=tf.zeros_initializer(), name='cf_bias')\n",
    "        #self.transu_bias = tf.get_variable(shape=[self.batch_size_cf,1], initializer=tf.zeros_initializer(), name='transu_bias')\n",
    "        return all_weights        \n",
    "    \n",
    "    def _build_model_cf(self):\n",
    "        #self.ua_embeddings, self.ea_embeddings = self._create_graphsage_embed()\n",
    "\n",
    "        self.u_e = tf.nn.embedding_lookup(self.weights['user_embed'], self.users)\n",
    "        \n",
    "        self.pos_i_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.pos_items)\n",
    "        self.neg_i_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neg_items)\n",
    "\n",
    "\n",
    "        #加入邻居居信息\n",
    "        self.u_e = self.fusion_neighbor() \n",
    "        \n",
    "        #投影\n",
    "        self.project_pos_i_e = tf.matmul(self.pos_i_e,self.weights['trans_U']) #+ self.transu_bias\n",
    "        self.project_neg_i_e = tf.matmul(self.neg_i_e,self.weights['trans_U']) #+ self.transu_bias\n",
    "        \n",
    "        self.batch_predictions = tf.reduce_sum(tf.multiply(self.u_e,self.project_pos_i_e), axis=1)\n",
    "        self.batch_predictions_normalized = tf.sigmoid(self.batch_predictions)\n",
    "    \n",
    "    @pysnooper.snoop(prefix='model')\n",
    "    def fusion_neighbor(self):\n",
    "\n",
    "        k = self.neighbors\n",
    "        k_shape = self.neighbors.shape\n",
    "        shape = [self.batch_size_cf,1]\n",
    "\n",
    "        self.neighbour_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neighbors[:,:,0])\n",
    "        self.neighbour_r = tf.nn.embedding_lookup(self.weights['relation_embed'],self.neighbors[:,:,1])\n",
    "         \n",
    "        n_iter = 2\n",
    "\n",
    "        fusion = self.fussion_class(self.emb_dim,self.neighbor_sample_size)\n",
    "        \n",
    "        # (self, neighbor_vectors, neighbor_relations, user_embeddings): [batch_size,  n_neighbor, dim]\n",
    "        new_embedding = fusion(\n",
    "                                neighbor_entities=self.neighbour_e,\n",
    "                                neighbor_relations=self.neighbour_r,\n",
    "                                user_embeddings=self.u_e)\n",
    " \n",
    "        return new_embedding #, aggregators        \n",
    "    \n",
    "    def _build_model_kg(self):\n",
    "        self.h_e, self.r_e, self.pos_t_e, self.neg_t_e = self._trnasR(self.h, self.r, self.pos_t, self.neg_t)\n",
    "     \n",
    "    def _trnasR(self, h, r, pos_t, neg_t):\n",
    "        embeddings = tf.expand_dims(self.weights['entity_embed'], 1)\n",
    "        # head & tail entity embeddings: batch_size *1 * emb_dim\n",
    "        h_e = tf.nn.embedding_lookup(embeddings, h)\n",
    "        pos_t_e = tf.nn.embedding_lookup(embeddings, pos_t)\n",
    "        neg_t_e = tf.nn.embedding_lookup(embeddings, neg_t)\n",
    "        # relation embeddings: batch_size * kge_dim\n",
    "        r_e = tf.nn.embedding_lookup(self.weights['relation_embed'], r)\n",
    "        # relation transform weights: batch_size * kge_dim * emb_dim\n",
    "        trans_W = tf.nn.embedding_lookup(self.weights['trans_W'], r)\n",
    " \n",
    "        h_e = tf.reshape(tf.matmul(h_e, trans_W), [-1, self.kge_dim])\n",
    "        pos_t_e = tf.reshape(tf.matmul(pos_t_e, trans_W), [-1, self.kge_dim])\n",
    "        neg_t_e = tf.reshape(tf.matmul(neg_t_e, trans_W), [-1, self.kge_dim])\n",
    "\n",
    "\n",
    "        return h_e, r_e, pos_t_e, neg_t_e\n",
    "\n",
    "\n",
    "    def _build_loss(self):\n",
    "\n",
    "        pos_scores = tf.reduce_mean(tf.multiply(self.u_e, self.project_pos_i_e), axis=1)\n",
    "        neg_scores = tf.reduce_mean(tf.multiply(self.u_e, self.project_neg_i_e), axis=1)\n",
    "\n",
    "        regularizer = tf.nn.l2_loss(self.u_e) + tf.nn.l2_loss(self.pos_i_e) + tf.nn.l2_loss(self.neg_i_e)\n",
    "        \n",
    "        regularizer = regularizer / self.batch_size_cf\n",
    "\n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        cf_loss = tf.reduce_mean(tf.nn.softplus(-(pos_scores - neg_scores)))\n",
    "        self.cf_reg_loss = self.regs[0] * regularizer\n",
    "        self.cf_loss = cf_loss+self.cf_reg_loss\n",
    "        \n",
    "        def _get_kg_score(h_e, r_e, t_e):\n",
    "            kg_score = tf.reduce_mean(tf.square((h_e + r_e - t_e)), 1, keepdims=True)\n",
    "            return kg_score\n",
    "\n",
    "        pos_kg_score = _get_kg_score(self.h_e, self.r_e, self.pos_t_e)\n",
    "        neg_kg_score = _get_kg_score(self.h_e, self.r_e, self.neg_t_e)\n",
    "        \n",
    "        # Using the softplus as BPR loss to avoid the nan error.\n",
    "        kg_loss = tf.reduce_mean(tf.nn.softplus(-(neg_kg_score - pos_kg_score)))\n",
    "\n",
    "\n",
    "\n",
    "        kg_reg_loss = tf.nn.l2_loss(self.h_e) + tf.nn.l2_loss(self.r_e) + \\\n",
    "                      tf.nn.l2_loss(self.pos_t_e) + tf.nn.l2_loss(self.neg_t_e)\n",
    "        kg_reg_loss = kg_reg_loss / self.batch_size_kg\n",
    "\n",
    "        self.kg_reg_loss = self.regs[1] * kg_reg_loss\n",
    "        self.kg_loss = kg_loss + self.kg_reg_loss\n",
    "\n",
    "        \n",
    "        self.loss = 0.8*self.cf_loss + 0.2*self.kg_loss \n",
    "\n",
    "        # Optimization process\n",
    "        self._opt = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        return sess.run([self._opt,self.loss,self.cf_loss,self.cf_reg_loss,self.kg_loss,self.kg_reg_loss], feed_dict)\n",
    "        \n",
    "    \n",
    "    #@pysnooper.snoop(prefix='recallK')\n",
    "    def recallK(self,y_true,y_pred,k):\n",
    "        def getTopK(y_pred,k):\n",
    "            topK=[]\n",
    "            a=y_pred\n",
    "            row_len = y_true.shape[0]\n",
    "            col_len = self.items\n",
    "            for i in range(row_len):\n",
    "                b = heapq.nlargest(k,range(col_len),a[i].take)\n",
    "                topK.append(b)\n",
    "            return topK\n",
    "        x=self.items\n",
    "        topK = getTopK(y_pred,k)\n",
    "        print(topK)\n",
    "\n",
    "        T=[]\n",
    "        for i in range(len(y_true)):\n",
    "            b = np.argwhere(y[i]==1)\n",
    "            b = np.squeeze(b)\n",
    "            T.append(b)\n",
    "        print(x)\n",
    "\n",
    "        recall_k = []\n",
    "        for i in range(len(topK)):\n",
    "            #print(topK[i])\n",
    "            #print(T[i])\n",
    "            recall_ = len(set(topK[i])&set(T[i]))/len(T[i])\n",
    "            recall_k.append(recall_)\n",
    "            print(recall/len(topK))\n",
    "            return np.array(recall_k)\n",
    "    \n",
    "    def eval_(self, sess, feed_dict):\n",
    "        labels, scores = sess.run([self.labels, self.batch_predictions_normalized], feed_dict)\n",
    "        #AUC\n",
    "        auc = roc_auc_score(y_true=labels, y_score=scores)\n",
    "        \n",
    "        #recall@K\n",
    "        #rk = self.recallK(labels, scores,3)\n",
    "        #F1\n",
    "        scores[scores >= 0.5] = 1\n",
    "        scores[scores < 0.5] = 0\n",
    "        f1 = f1_score(y_true=labels, y_pred=scores)\n",
    "        \n",
    "        return auc, f1\n",
    "\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.n_users,self.test_data,self.weights)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self,args):\n",
    "        self.args=args\n",
    "        self.batch_size_cf = args.batch_size_cf\n",
    "        self.batch_size_kg = args.batch_size_kg\n",
    "        self.mess_dropout = args.mess_dropout\n",
    "        \n",
    "    #@pysnooper.snoop(prefix='ctr_eval')\n",
    "    def ctr_eval(self,sess, model, data,flag):\n",
    "        start = 0\n",
    "        auc_list = []\n",
    "        f1_list = []\n",
    "        rk_list = []\n",
    "        _batch = self.batch_size_cf//2\n",
    "        while start + _batch <= data.shape[0]:\n",
    "            auc, f1 = model.eval_(sess, self.get_test_feed_dict(model, data, start, start + _batch,flag))\n",
    "            auc_list.append(auc)\n",
    "            f1_list.append(f1)\n",
    "            \n",
    "            start += _batch\n",
    "        return float(np.mean(auc_list)), float(np.mean(f1_list))\n",
    "        \n",
    "    #@pysnooper.snoop(prefix='get_test_feed_dict')\n",
    "    def get_test_feed_dict(self,model, data, start, end,flag):\n",
    "        neg_items = []\n",
    "        users = data[start:end,0]\n",
    "        pos_items = data[start:end,1]\n",
    "        label = data[start:end,2]\n",
    "        \n",
    "        n_user=len(users)\n",
    "        all_user = np.squeeze(np.tile(users,(2,1)).reshape(-1,2*n_user))\n",
    "        \n",
    "        for u in users:\n",
    "            #pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += self.sample_neg_items_for_u(u, 1,flag)\n",
    "        all_item = np.squeeze(np.vstack((pos_items,neg_items)).reshape(-1,2*n_user))\n",
    "        all_label = np.pad(label,(0,n_user),'constant',constant_values=(0,0))\n",
    "        \n",
    "        neighbors = cdata.construct_neighbours(pos_items)\n",
    "        all_neighbors = np.vstack((neighbors,neighbors)).reshape(-1,self.args.neighbor_sample_size,2)\n",
    "        feed_dict = {\n",
    "            model.users: all_user,\n",
    "            model.pos_items: all_item,\n",
    "            model.neg_items: neg_items,\n",
    "            model.labels:all_label,\n",
    "            model.neighbors:all_neighbors,\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def sample_neg_items_for_u(self,u, num,flag):\n",
    "        neg_items = []\n",
    "        if(flag=='train'):\n",
    "            _user_dict = self.train_user_dict\n",
    "        if(flag=='eval'):\n",
    "            _user_dict = self.eval_user_dict\n",
    "        if(flag=='test'):\n",
    "            _user_dict = self.test_user_dict\n",
    "        while True:\n",
    "            if len(neg_items) == num: break\n",
    "            neg_i_id = np.random.randint(low=0, high=self.n_item,size=1)[0]\n",
    "\n",
    "            if neg_i_id not in _user_dict[u] and neg_i_id not in neg_items:\n",
    "                neg_items.append(neg_i_id)\n",
    "        return neg_items\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    def load_data(self):\n",
    "        self.n_user,self.n_item, self.train_data, self.eval_data, self.test_data,self.exist_users,self.train_user_dict,self.test_user_dict,self.eval_user_dict,self.n_interaction = self.load_rating()\n",
    "        #n_entity, n_relation, adj_entity, adj_relation = load_kg(args)\n",
    "        self.n_entity, self.n_relation, self.kg,self.rd,self.triplet_number = self.load_kg()\n",
    "        print('data loaded.')\n",
    "\n",
    "        return self.n_user, self.n_item, self.n_entity, self.n_relation, self.train_data, self.eval_data, self.test_data, self.kg,self.rd,self.n_interaction,self.triplet_number\n",
    "\n",
    "    #@pysnooper.snoop(prefix='load_kg')\n",
    "    def load_rating(self):\n",
    "        print('reading rating file ...')\n",
    "\n",
    "        # reading rating file\n",
    "        rating_file = './data/' + self.args.dataset + '/ratings_final'\n",
    "        if os.path.exists(rating_file + '.npy'):\n",
    "            rating_np = np.load(rating_file + '.npy')\n",
    "        else:\n",
    "            rating_np = np.loadtxt(rating_file + '.txt', dtype=np.int64)\n",
    "            np.save(rating_file + '.npy', rating_np)\n",
    "\n",
    "        n_user = len(set(rating_np[:, 0]))\n",
    "        n_item = len(set(rating_np[:, 1]))\n",
    "        \n",
    "        n_interaction = len(rating_np)\n",
    "        \n",
    "        \n",
    "        train_data, eval_data, test_data = self.dataset_split(rating_np)\n",
    "\n",
    "        #exist_users=list(set(train_data[:,0]))\n",
    "        all_users=list(set(rating_np[:,0]))\n",
    "        \n",
    "        # {userid:[pos_id...]}}\n",
    "        train_user_dict=dict()\n",
    "        for id in range(len(train_data)):\n",
    "            u_id = train_data[id][0]\n",
    "            i_id = train_data[id][1]\n",
    "            if u_id not in train_user_dict.keys():\n",
    "                train_user_dict[u_id] = []\n",
    "            train_user_dict[u_id].append(i_id)\n",
    "        \n",
    "        test_user_dict=dict()\n",
    "        for id in range(len(test_data)):\n",
    "            u_id = test_data[id][0]\n",
    "            i_id = test_data[id][1]\n",
    "            if u_id not in test_user_dict.keys():\n",
    "                test_user_dict[u_id] = []\n",
    "            test_user_dict[u_id].append(i_id)        \n",
    "            \n",
    "        eval_user_dict=dict()\n",
    "        for id in range(len(eval_data)):\n",
    "            u_id = eval_data[id][0]\n",
    "            i_id = eval_data[id][1]\n",
    "            if u_id not in eval_user_dict.keys():\n",
    "                eval_user_dict[u_id] = []\n",
    "            eval_user_dict[u_id].append(i_id)        \n",
    "            \n",
    "        return n_user, n_item, train_data, eval_data, test_data,all_users,train_user_dict,test_user_dict,eval_user_dict,n_interaction\n",
    "\n",
    "\n",
    "    def dataset_split(self,rating_np):\n",
    "        print('splitting dataset ...')\n",
    "\n",
    "        # train:eval:test = 6:2:2\n",
    "        eval_ratio = 0.2\n",
    "        test_ratio = 0.2\n",
    "        n_ratings = rating_np.shape[0]\n",
    "\n",
    "        eval_indices = np.random.choice(list(range(n_ratings)), size=int(n_ratings * eval_ratio), replace=False)\n",
    "        left = set(range(n_ratings)) - set(eval_indices)\n",
    "        test_indices = np.random.choice(list(left), size=int(n_ratings * test_ratio), replace=False)\n",
    "        train_indices = list(left - set(test_indices))\n",
    "        if self.args.ratio < 1:\n",
    "            train_indices = np.random.choice(list(train_indices), size=int(len(train_indices) * self.args.ratio), replace=False)\n",
    "\n",
    "        train_data = rating_np[train_indices]\n",
    "        eval_data = rating_np[eval_indices]\n",
    "        test_data = rating_np[test_indices]\n",
    "\n",
    "        \n",
    "        return train_data, eval_data, test_data\n",
    "\n",
    "\n",
    "    def load_kg(self):\n",
    "        print('reading KG file ...')\n",
    "\n",
    "        # reading kg file\n",
    "        kg_file = './data/' + self.args.dataset + '/kg_final'\n",
    "        if os.path.exists(kg_file + '.npy'):\n",
    "            kg_np = np.load(kg_file + '.npy')\n",
    "        else:\n",
    "            kg_np = np.loadtxt(kg_file + '.txt', dtype=np.int64)\n",
    "            np.save(kg_file + '.npy', kg_np)\n",
    "\n",
    "        n_entity = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        n_relation = len(set(kg_np[:, 1]))\n",
    "        \n",
    "        triplet_number = len(kg_np)\n",
    "\n",
    "        kg,rd = self.construct_kg(kg_np)\n",
    "\n",
    "        return n_entity, n_relation, kg,rd ,triplet_number\n",
    "\n",
    "\n",
    "    def construct_kg(self,kg_np):\n",
    "        print('constructing knowledge graph ...')\n",
    "        kg = dict()\n",
    "        rd = dict()\n",
    "        for triple in kg_np:\n",
    "            head = triple[0]\n",
    "            relation = triple[1]\n",
    "            tail = triple[2]\n",
    "            # treat the KG as an undirected graph\n",
    "            if head not in kg.keys():\n",
    "                kg[head] = []\n",
    "            kg[head].append((tail, relation))\n",
    "            #if tail not in kg:\n",
    "            #    kg[tail] = []\n",
    "            #kg[tail].append((head, relation))\n",
    "            if relation not in rd.keys():\n",
    "                rd[relation]=[]\n",
    "            rd[relation].append((head,tail))\n",
    "        return kg,rd\n",
    "\n",
    "\n",
    "    def generate_train_feed_dict(self, model,start,end):\n",
    "        self.generate_cf_batch(start,end)\n",
    "        self.generate_kg_batch()\n",
    "        feed_dict = {\n",
    "            model.h: self.batch_data['heads'],\n",
    "            model.r: self.batch_data['relations'],\n",
    "            model.pos_t: self.batch_data['pos_tails'],\n",
    "            model.neg_t: self.batch_data['neg_tails'],\n",
    "\n",
    "            model.users: self.batch_data['users'],\n",
    "            model.pos_items: self.batch_data['pos_items'],\n",
    "            model.neg_items: self.batch_data['neg_items'],\n",
    "\n",
    "            model.neighbors: self.batch_data['neighbors'],\n",
    "            model.mess_dropout: eval(self.args.mess_dropout),\n",
    "            model.node_dropout: eval(self.args.node_dropout),\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "    \n",
    "   \n",
    "    def generate_cf_batch(self,start,end):\n",
    "        \n",
    "        users, pos_items, neg_items,neighbors = self._generate_train_cf_batch2(start,end)\n",
    "\n",
    "        self.batch_data = {}\n",
    "        self.batch_data['users'] = users\n",
    "        self.batch_data['pos_items'] = pos_items\n",
    "        self.batch_data['neg_items'] = neg_items\n",
    "        \n",
    "        self.batch_data['heads'] = pos_items\n",
    "        self.batch_data['neighbors'] = neighbors\n",
    "    \n",
    "    def generate_kg_batch(self):\n",
    "        heads, relations, pos_tails, neg_tails = self._generate_kg_batch()\n",
    "        self.batch_data['relations'] = relations\n",
    "        self.batch_data['pos_tails'] = pos_tails\n",
    "        self.batch_data['neg_tails'] = neg_tails\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _generate_train_cf_batch2(self,start,end):\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_item,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        users = self.train_data[start:end,0]\n",
    "        pos_items = self.train_data[start:end,1]\n",
    "        for u in users:\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        neighbors = cdata.construct_neighbours(pos_items)\n",
    "        return users, pos_items, neg_items,neighbors\n",
    "   \n",
    "    def _generate_kg_batch(self):\n",
    "        exist_heads = self.kg.keys()\n",
    "\n",
    "        heads = self.batch_data['pos_items']\n",
    "\n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.kg[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_entity, size=1)[0]\n",
    "                if (t, r) not in self.kg[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "\n",
    "        return heads, pos_r_batch, pos_t_batch, neg_t_batch\n",
    "        \n",
    "\n",
    "    def construct_neighbours(self,pos_items):\n",
    "        neighbor_sample_size=self.args.neighbor_sample_size\n",
    "        head_items = np.unique(pos_items)\n",
    "        end = np.zeros((len(pos_items),neighbor_sample_size,2))  # 2 (end relation)\n",
    "        for headid in head_items:\n",
    "\n",
    "            elements = np.array(self.kg[headid])\n",
    "            numbers = len(elements)\n",
    "            if(numbers>=neighbor_sample_size):\n",
    "                sampled_indices = np.random.choice(list(range(numbers)),size=neighbor_sample_size,replace=False)\n",
    "            else:\n",
    "                sampled_indices = np.random.choice(list(range(numbers)),size=neighbor_sample_size,replace=True)\n",
    "\n",
    "            idx = np.argwhere(pos_items == headid)\n",
    "            idx = np.array(idx.reshape(1, -1).squeeze(0))\n",
    "\n",
    "            end[idx,:] = elements[sampled_indices,:]\n",
    "        return end.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading rating file ...\n",
      "splitting dataset ...\n",
      "reading KG file ...\n",
      "constructing knowledge graph ...\n",
      "data loaded.\n"
     ]
    }
   ],
   "source": [
    "cdata = Data(args)\n",
    "data =cdata.load_data()\n",
    "# 0       1        2           3          4          5            6      7  8    9              10\n",
    "#n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, kg,rd,n_interaction,triplet_number\n",
    "\n",
    "#print(data)\n",
    "\n",
    "#print (data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:1872,item:3846,interactions:21173,entity:9366,relation:60,triplets:15518\n",
      "12705 4234 4234\n",
      "21173\n",
      "3846\n",
      "21173\n"
     ]
    }
   ],
   "source": [
    "print(\"user:%d,item:%d,interactions:%d,entity:%d,relation:%d,triplets:%d\"%(data[0],data[1],data[9],data[2],data[3],data[10]))\n",
    "print(data[4].shape[0],data[5].shape[0],data[6].shape[0])\n",
    "print(data[4].shape[0]+data[5].shape[0]+data[6].shape[0])\n",
    "print(len(data[7]))\n",
    "print(data[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "k=data[7]\n",
    "#print(k.keys())\n",
    "#print(k[67])\n",
    "#print(k[14967])\n",
    "#print(len(k[14967]))\n",
    "#print(len(k[15061]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users, pos_items, neg_items,neighbors= cdata._generate_train_cf_batch2(1,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using xavier initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modelSource path:... <ipython-input-4-4fa9ab888a1d>\n",
      "modelStarting var:.. self = <__main__.RGAT object at 0x7fd570ef89b0>\n",
      "model15:05:18.954383 call       106     def fusion_neighbor(self):\n",
      "model15:05:18.954630 line       108         k = self.neighbors\n",
      "modelNew var:....... k = <tf.Tensor 'neighbours:0' shape=(?, 1, 2) dtype=int32>\n",
      "model15:05:18.954698 line       109         k_shape = self.neighbors.shape\n",
      "modelNew var:....... k_shape = TensorShape([Dimension(None), Dimension(1), Dimension(2)])\n",
      "model15:05:18.954838 line       110         shape = [self.batch_size_cf,1]\n",
      "modelNew var:....... shape = [2048, 1]\n",
      "model15:05:18.954950 line       112         self.neighbour_e = tf.nn.embedding_lookup(self.weights['entity_embed'], self.neighbors[:,:,0])\n",
      "model15:05:18.966321 line       113         self.neighbour_r = tf.nn.embedding_lookup(self.weights['relation_embed'],self.neighbors[:,:,1])\n",
      "model15:05:18.972690 line       115         n_iter = 2\n",
      "modelNew var:....... n_iter = 2\n",
      "model15:05:18.972831 line       117         fusion = self.fussion_class(self.emb_dim,self.neighbor_sample_size)\n",
      "fusion_init    Source path:... <ipython-input-3-58b8e7b2dbf8>\n",
      "fusion_init    Starting var:.. self = <__main__.CNNFusion object at 0x7fd570a77ac8>\n",
      "fusion_init    Starting var:.. emb_dim = 64\n",
      "fusion_init    Starting var:.. neighbor_sample_size = 1\n",
      "fusion_init    15:05:18.972988 call         5     def __init__(self,emb_dim,neighbor_sample_size):\n",
      "fusion_init    15:05:18.973210 line         6         self.neighbor_sample_size = neighbor_sample_size\n",
      "fusion_init    15:05:18.973278 line         7         self.emb_dim = emb_dim\n",
      "fusion_init    15:05:18.973340 line         8         x = self.neighbor_sample_size\n",
      "fusion_init    New var:....... x = 1\n",
      "fusion_init    15:05:18.973399 line         9         y = self.emb_dim\n",
      "fusion_init    New var:....... y = 64\n",
      "fusion_init    15:05:18.973501 return       9         y = self.emb_dim\n",
      "fusion_init    Return value:.. None\n",
      "fusion_init    Elapsed time: 00:00:00.000640\n",
      "modelNew var:....... fusion = <__main__.CNNFusion object at 0x7fd570a77ac8>\n",
      "model15:05:18.973718 line       120         new_embedding = fusion(\n",
      "model15:05:18.973849 line       121                                 neighbor_entities=self.neighbour_e,\n",
      "model15:05:18.973950 line       122                                 neighbor_relations=self.neighbour_r,\n",
      "model15:05:18.974080 line       123                                 user_embeddings=self.u_e)\n",
      "cnn    Source path:... <ipython-input-3-58b8e7b2dbf8>\n",
      "cnn    Starting var:.. self = <__main__.CNNFusion object at 0x7fd570a77ac8>\n",
      "cnn    Starting var:.. neighbor_entities = <tf.Tensor 'embedding_lookup_3/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "cnn    Starting var:.. neighbor_relations = <tf.Tensor 'embedding_lookup_4/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "cnn    Starting var:.. user_embeddings = <tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 64) dtype=float32>\n",
      "cnn    15:05:18.974198 call       111     def fusion(self, neighbor_entities, neighbor_relations, user_embeddings):\n",
      "cnn    15:05:18.974457 line       112         neighbors_fusion = self._cnn_fusion_neighbor_entities(neighbor_entities, neighbor_relations)\n",
      "mix        Source path:... <ipython-input-3-58b8e7b2dbf8>\n",
      "mix        Starting var:.. self = <__main__.CNNFusion object at 0x7fd570a77ac8>\n",
      "mix        Starting var:.. neighbor_entities = <tf.Tensor 'embedding_lookup_3/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        Starting var:.. neighbor_relations = <tf.Tensor 'embedding_lookup_4/Identity:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        15:05:18.974576 call        52     def _cnn_fusion_neighbor_entities(self, neighbor_entities, neighbor_relations):\n",
      "mix        15:05:18.974744 line        54         E_R = tf.matmul(tf.transpose(neighbor_relations, perm=[0,2,1]),neighbor_entities)\n",
      "mix        New var:....... E_R = <tf.Tensor 'MatMul:0' shape=(?, 64, 64) dtype=float32>\n",
      "mix        15:05:18.978332 line        55         E_R = tf.expand_dims(E_R, -1)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'ExpandDims:0' shape=(?, 64, 64, 1) dtype=float32>\n",
      "mix        15:05:18.980649 line        56         conv2D_1 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_1 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd570a87080>\n",
      "mix        15:05:19.021613 line        58         E_R = conv2D_1(E_R)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d/BiasAdd:0' shape=(?, 32, 32, 16) dtype=float32>\n",
      "mix        15:05:19.074447 line        59         conv2D_2 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_2 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd570a29978>\n",
      "mix        15:05:19.079520 line        60         E_R = conv2D_2(E_R)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d_1/BiasAdd:0' shape=(?, 16, 16, 16) dtype=float32>\n",
      "mix        15:05:19.103876 line        61         conv2D_3 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_3 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd570a29cf8>\n",
      "mix        15:05:19.108617 line        62         E_R = conv2D_3(E_R)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d_2/BiasAdd:0' shape=(?, 8, 8, 16) dtype=float32>\n",
      "mix        15:05:19.133145 line        64         conv2D_4 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_4 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd5709f2ac8>\n",
      "mix        15:05:19.137911 line        65         E_R = conv2D_4(E_R)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d_3/BiasAdd:0' shape=(?, 4, 4, 16) dtype=float32>\n",
      "mix        15:05:19.162827 line        67         conv2D_5 = tf.keras.layers.Conv2D(16,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_5 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd570a02710>\n",
      "mix        15:05:19.167886 line        68         E_R = conv2D_5(E_R)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d_4/BiasAdd:0' shape=(?, 2, 2, 16) dtype=float32>\n",
      "mix        15:05:19.192773 line        70         conv2D_6 = tf.keras.layers.Conv2D(1,4,strides=(2,2),padding='same',data_format=None)\n",
      "mix        New var:....... conv2D_6 = <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fd570a10470>\n",
      "mix        15:05:19.197970 line        71         E_R = conv2D_6(E_R)\n",
      "mix        Modified var:.. E_R = <tf.Tensor 'conv2d_5/BiasAdd:0' shape=(?, 1, 1, 1) dtype=float32>\n",
      "mix        15:05:19.223125 line        73         _relation_scores = tf.squeeze(E_R, axis=2)\n",
      "mix        New var:....... _relation_scores = <tf.Tensor 'Squeeze:0' shape=(?, 1, 1) dtype=float32>\n",
      "mix        15:05:19.224709 line        74         _relation_scores_normalized = tf.nn.softmax(_relation_scores, dim=-1)\n",
      "mix        New var:....... _relation_scores_normalized = <tf.Tensor 'Softmax:0' shape=(?, 1, 1) dtype=float32>\n",
      "mix        15:05:19.227306 line        75         neighbors_fusion = _relation_scores_normalized * neighbor_entities\n",
      "mix        New var:....... neighbors_fusion = <tf.Tensor 'mul:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        15:05:19.228819 line        77         conv1D = tf.keras.layers.Conv1D(self.emb_dim,self.neighbor_sample_size,self.neighbor_sample_size,padding='valid')\n",
      "mix        New var:....... conv1D = <tensorflow.python.keras.layers.convolutional.Conv1D object at 0x7fd570998ef0>\n",
      "mix        15:05:19.234050 line        78         neighbors_fusion = conv1D(neighbors_fusion)\n",
      "mix        Modified var:.. neighbors_fusion = <tf.Tensor 'conv1d/BiasAdd:0' shape=(?, 1, 64) dtype=float32>\n",
      "mix        15:05:19.264122 line        80         neighbors_fusion = tf.squeeze(neighbors_fusion,axis=1)\n",
      "mix        Modified var:.. neighbors_fusion = <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "mix        15:05:19.265569 line        82         return neighbors_fusion    \n",
      "mix        15:05:19.266000 return      82         return neighbors_fusion    \n",
      "mix        Return value:.. <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "mix        Elapsed time: 00:00:00.291782\n",
      "cnn    New var:....... neighbors_fusion = <tf.Tensor 'Squeeze_1:0' shape=(?, 64) dtype=float32>\n",
      "cnn    15:05:19.266423 line       117         return self.act(neighbors_fusion)       \n",
      "cnn    15:05:19.267471 return     117         return self.act(neighbors_fusion)       \n",
      "cnn    Return value:.. <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "cnn    Elapsed time: 00:00:00.293616\n",
      "modelNew var:....... new_embedding = <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "model15:05:19.267865 line       125         return new_embedding #, aggregators        \n",
      "model15:05:19.267998 return     125         return new_embedding #, aggregators        \n",
      "modelReturn value:.. <tf.Tensor 'Relu:0' shape=(?, 64) dtype=float32>\n",
      "modelElapsed time: 00:00:00.313782\n"
     ]
    }
   ],
   "source": [
    "model = RGAT(data,args,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'   #指定第一块GPU可用\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.8  # 程序最多只能占用指定gpu50%的显存\n",
    "config.gpu_options.allow_growth = True      #程序按需申请内存\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 [3.9s]: [all_loss:4.85201, cf_loss:4.85202, cf_reg_loss:0.00000, kge_loss:4.85199, kge_reg_loss 0.00003 ]\n",
      "---epoch 0    train auc: 0.6551  f1: 0.6423    eval auc: 0.6456  f1: 0.6329    test auc: 0.6496  f1: 0.6383\n",
      "Epoch 1 [1.8s]: [all_loss:4.85191, cf_loss:4.85196, cf_reg_loss:0.00000, kge_loss:4.85168, kge_reg_loss 0.00003 ]\n",
      "---epoch 1    train auc: 0.7555  f1: 0.6989    eval auc: 0.7500  f1: 0.6921    test auc: 0.7431  f1: 0.6894\n",
      "Epoch 2 [1.7s]: [all_loss:4.85172, cf_loss:4.85188, cf_reg_loss:0.00000, kge_loss:4.85109, kge_reg_loss 0.00003 ]\n",
      "---epoch 2    train auc: 0.8039  f1: 0.7252    eval auc: 0.7900  f1: 0.7116    test auc: 0.7878  f1: 0.7115\n",
      "Epoch 3 [1.6s]: [all_loss:4.85140, cf_loss:4.85174, cf_reg_loss:0.00000, kge_loss:4.85000, kge_reg_loss 0.00004 ]\n",
      "---epoch 3    train auc: 0.8298  f1: 0.7402    eval auc: 0.8081  f1: 0.7224    test auc: 0.8053  f1: 0.7233\n",
      "Epoch 4 [1.9s]: [all_loss:4.85082, cf_loss:4.85150, cf_reg_loss:0.00000, kge_loss:4.84807, kge_reg_loss 0.00004 ]\n",
      "---epoch 4    train auc: 0.8420  f1: 0.7515    eval auc: 0.8167  f1: 0.7314    test auc: 0.8092  f1: 0.7279\n",
      "Epoch 5 [1.6s]: [all_loss:4.84987, cf_loss:4.85107, cf_reg_loss:0.00000, kge_loss:4.84508, kge_reg_loss 0.00004 ]\n",
      "---epoch 5    train auc: 0.8467  f1: 0.7652    eval auc: 0.8231  f1: 0.7469    test auc: 0.8143  f1: 0.7381\n",
      "Epoch 6 [1.7s]: [all_loss:4.84834, cf_loss:4.85029, cf_reg_loss:0.00000, kge_loss:4.84055, kge_reg_loss 0.00004 ]\n",
      "---epoch 6    train auc: 0.8534  f1: 0.7758    eval auc: 0.8188  f1: 0.7497    test auc: 0.8102  f1: 0.7394\n",
      "Epoch 7 [1.6s]: [all_loss:4.84602, cf_loss:4.84893, cf_reg_loss:0.00000, kge_loss:4.83438, kge_reg_loss 0.00004 ]\n",
      "---epoch 7    train auc: 0.8542  f1: 0.7726    eval auc: 0.8237  f1: 0.7526    test auc: 0.8108  f1: 0.7394\n",
      "Epoch 8 [2.0s]: [all_loss:4.84243, cf_loss:4.84650, cf_reg_loss:0.00001, kge_loss:4.82614, kge_reg_loss 0.00005 ]\n",
      "---epoch 8    train auc: 0.8531  f1: 0.7727    eval auc: 0.8156  f1: 0.7486    test auc: 0.8110  f1: 0.7419\n",
      "Epoch 10 [1.6s]: [all_loss:4.82907, cf_loss:4.83538, cf_reg_loss:0.00002, kge_loss:4.80384, kge_reg_loss 0.00006 ]\n",
      "---epoch 10    train auc: 0.8533  f1: 0.7682    eval auc: 0.8230  f1: 0.7494    test auc: 0.8128  f1: 0.7389\n",
      "Epoch 11 [1.6s]: [all_loss:4.81696, cf_loss:4.82399, cf_reg_loss:0.00002, kge_loss:4.78886, kge_reg_loss 0.00007 ]\n",
      "---epoch 11    train auc: 0.8558  f1: 0.7701    eval auc: 0.8197  f1: 0.7460    test auc: 0.8209  f1: 0.7438\n",
      "Epoch 12 [1.7s]: [all_loss:4.79934, cf_loss:4.80615, cf_reg_loss:0.00004, kge_loss:4.77208, kge_reg_loss 0.00008 ]\n",
      "---epoch 12    train auc: 0.8561  f1: 0.7663    eval auc: 0.8238  f1: 0.7434    test auc: 0.8172  f1: 0.7405\n",
      "Epoch 13 [1.7s]: [all_loss:4.77377, cf_loss:4.77968, cf_reg_loss:0.00006, kge_loss:4.75016, kge_reg_loss 0.00009 ]\n",
      "---epoch 13    train auc: 0.8582  f1: 0.7680    eval auc: 0.8227  f1: 0.7441    test auc: 0.8185  f1: 0.7410\n",
      "Epoch 14 [1.7s]: [all_loss:4.73865, cf_loss:4.74220, cf_reg_loss:0.00008, kge_loss:4.72447, kge_reg_loss 0.00011 ]\n",
      "---epoch 14    train auc: 0.8599  f1: 0.7682    eval auc: 0.8314  f1: 0.7486    test auc: 0.8290  f1: 0.7453\n",
      "Epoch 15 [1.7s]: [all_loss:4.68641, cf_loss:4.68495, cf_reg_loss:0.00012, kge_loss:4.69222, kge_reg_loss 0.00013 ]\n",
      "---epoch 15    train auc: 0.8614  f1: 0.7678    eval auc: 0.8300  f1: 0.7419    test auc: 0.8256  f1: 0.7394\n",
      "Epoch 16 [1.9s]: [all_loss:4.61842, cf_loss:4.60977, cf_reg_loss:0.00017, kge_loss:4.65301, kge_reg_loss 0.00015 ]\n",
      "---epoch 16    train auc: 0.8620  f1: 0.7644    eval auc: 0.8402  f1: 0.7499    test auc: 0.8186  f1: 0.7329\n",
      "Epoch 17 [1.6s]: [all_loss:4.52264, cf_loss:4.50313, cf_reg_loss:0.00024, kge_loss:4.60068, kge_reg_loss 0.00018 ]\n",
      "---epoch 17    train auc: 0.8610  f1: 0.7625    eval auc: 0.8365  f1: 0.7437    test auc: 0.8288  f1: 0.7361\n",
      "Epoch 18 [1.8s]: [all_loss:4.40570, cf_loss:4.37350, cf_reg_loss:0.00032, kge_loss:4.53450, kge_reg_loss 0.00022 ]\n",
      "---epoch 18    train auc: 0.8634  f1: 0.7656    eval auc: 0.8303  f1: 0.7404    test auc: 0.8323  f1: 0.7394\n",
      "Epoch 20 [1.6s]: [all_loss:4.09076, cf_loss:4.02549, cf_reg_loss:0.00057, kge_loss:4.35186, kge_reg_loss 0.00032 ]\n",
      "---epoch 20    train auc: 0.8622  f1: 0.7617    eval auc: 0.8362  f1: 0.7435    test auc: 0.8299  f1: 0.7359\n",
      "Epoch 21 [1.6s]: [all_loss:3.89126, cf_loss:3.80516, cf_reg_loss:0.00075, kge_loss:4.23563, kge_reg_loss 0.00040 ]\n",
      "---epoch 21    train auc: 0.8623  f1: 0.7633    eval auc: 0.8385  f1: 0.7468    test auc: 0.8249  f1: 0.7320\n",
      "Epoch 22 [1.7s]: [all_loss:3.68774, cf_loss:3.58565, cf_reg_loss:0.00096, kge_loss:4.09613, kge_reg_loss 0.00049 ]\n",
      "---epoch 22    train auc: 0.8590  f1: 0.7601    eval auc: 0.8341  f1: 0.7427    test auc: 0.8280  f1: 0.7366\n",
      "Epoch 23 [1.7s]: [all_loss:3.47700, cf_loss:3.35916, cf_reg_loss:0.00119, kge_loss:3.94836, kge_reg_loss 0.00060 ]\n",
      "---epoch 23    train auc: 0.8589  f1: 0.7618    eval auc: 0.8335  f1: 0.7412    test auc: 0.8219  f1: 0.7339\n",
      "Epoch 24 [1.6s]: [all_loss:3.25276, cf_loss:3.12112, cf_reg_loss:0.00147, kge_loss:3.77931, kge_reg_loss 0.00073 ]\n",
      "---epoch 24    train auc: 0.8558  f1: 0.7613    eval auc: 0.8330  f1: 0.7453    test auc: 0.8229  f1: 0.7352\n",
      "Epoch 25 [1.7s]: [all_loss:3.04798, cf_loss:2.91177, cf_reg_loss:0.00177, kge_loss:3.59285, kge_reg_loss 0.00089 ]\n",
      "---epoch 25    train auc: 0.8582  f1: 0.7654    eval auc: 0.8302  f1: 0.7429    test auc: 0.8217  f1: 0.7360\n",
      "Epoch 26 [1.7s]: [all_loss:2.89929, cf_loss:2.76880, cf_reg_loss:0.00206, kge_loss:3.42125, kge_reg_loss 0.00105 ]\n",
      "---epoch 26    train auc: 0.8526  f1: 0.7634    eval auc: 0.8326  f1: 0.7494    test auc: 0.8240  f1: 0.7415\n",
      "Epoch 27 [1.7s]: [all_loss:2.72310, cf_loss:2.58841, cf_reg_loss:0.00236, kge_loss:3.26189, kge_reg_loss 0.00124 ]\n",
      "---epoch 27    train auc: 0.8527  f1: 0.7666    eval auc: 0.8267  f1: 0.7466    test auc: 0.8231  f1: 0.7451\n",
      "Epoch 28 [1.7s]: [all_loss:2.59391, cf_loss:2.47802, cf_reg_loss:0.00269, kge_loss:3.05746, kge_reg_loss 0.00144 ]\n",
      "---epoch 28    train auc: 0.8566  f1: 0.7726    eval auc: 0.8297  f1: 0.7554    test auc: 0.8138  f1: 0.7414\n",
      "Epoch 30 [1.6s]: [all_loss:2.38924, cf_loss:2.29656, cf_reg_loss:0.00326, kge_loss:2.75994, kge_reg_loss 0.00187 ]\n",
      "---epoch 30    train auc: 0.8544  f1: 0.7786    eval auc: 0.8233  f1: 0.7600    test auc: 0.8224  f1: 0.7581\n",
      "Epoch 31 [1.8s]: [all_loss:2.33369, cf_loss:2.25397, cf_reg_loss:0.00350, kge_loss:2.65255, kge_reg_loss 0.00208 ]\n",
      "---epoch 31    train auc: 0.8523  f1: 0.7793    eval auc: 0.8212  f1: 0.7614    test auc: 0.8203  f1: 0.7576\n",
      "Epoch 32 [1.6s]: [all_loss:2.26198, cf_loss:2.19617, cf_reg_loss:0.00373, kge_loss:2.52522, kge_reg_loss 0.00230 ]\n",
      "---epoch 32    train auc: 0.8502  f1: 0.7811    eval auc: 0.8186  f1: 0.7586    test auc: 0.8220  f1: 0.7631\n",
      "Epoch 33 [1.9s]: [all_loss:2.22256, cf_loss:2.17723, cf_reg_loss:0.00394, kge_loss:2.40391, kge_reg_loss 0.00251 ]\n",
      "---epoch 33    train auc: 0.8519  f1: 0.7836    eval auc: 0.8254  f1: 0.7661    test auc: 0.8142  f1: 0.7583\n",
      "Epoch 34 [1.7s]: [all_loss:2.14961, cf_loss:2.10500, cf_reg_loss:0.00411, kge_loss:2.32806, kge_reg_loss 0.00272 ]\n",
      "---epoch 34    train auc: 0.8516  f1: 0.7853    eval auc: 0.8249  f1: 0.7672    test auc: 0.8164  f1: 0.7622\n",
      "Epoch 35 [1.8s]: [all_loss:2.11241, cf_loss:2.07534, cf_reg_loss:0.00429, kge_loss:2.26069, kge_reg_loss 0.00291 ]\n",
      "---epoch 35    train auc: 0.8542  f1: 0.7904    eval auc: 0.8221  f1: 0.7690    test auc: 0.8160  f1: 0.7627\n",
      "Epoch 36 [1.6s]: [all_loss:2.05641, cf_loss:2.02849, cf_reg_loss:0.00440, kge_loss:2.16808, kge_reg_loss 0.00310 ]\n",
      "---epoch 36    train auc: 0.8473  f1: 0.7869    eval auc: 0.8166  f1: 0.7636    test auc: 0.8159  f1: 0.7643\n",
      "Epoch 37 [2.0s]: [all_loss:2.03417, cf_loss:2.01510, cf_reg_loss:0.00460, kge_loss:2.11047, kge_reg_loss 0.00329 ]\n",
      "---epoch 37    train auc: 0.8484  f1: 0.7888    eval auc: 0.8184  f1: 0.7689    test auc: 0.8175  f1: 0.7660\n",
      "Epoch 38 [1.6s]: [all_loss:2.04726, cf_loss:2.04203, cf_reg_loss:0.00471, kge_loss:2.06815, kge_reg_loss 0.00346 ]\n",
      "---epoch 38    train auc: 0.8476  f1: 0.7897    eval auc: 0.8159  f1: 0.7687    test auc: 0.8116  f1: 0.7658\n",
      "Epoch 40 [1.6s]: [all_loss:1.98956, cf_loss:1.99623, cf_reg_loss:0.00492, kge_loss:1.96286, kge_reg_loss 0.00379 ]\n",
      "---epoch 40    train auc: 0.8488  f1: 0.7927    eval auc: 0.8187  f1: 0.7734    test auc: 0.8130  f1: 0.7683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 [1.8s]: [all_loss:1.95696, cf_loss:1.96897, cf_reg_loss:0.00492, kge_loss:1.90891, kge_reg_loss 0.00395 ]\n",
      "---epoch 41    train auc: 0.8488  f1: 0.7939    eval auc: 0.8215  f1: 0.7770    test auc: 0.8106  f1: 0.7692\n",
      "Epoch 42 [1.7s]: [all_loss:1.90978, cf_loss:1.92086, cf_reg_loss:0.00503, kge_loss:1.86544, kge_reg_loss 0.00411 ]\n",
      "---epoch 42    train auc: 0.8513  f1: 0.7966    eval auc: 0.8162  f1: 0.7727    test auc: 0.8111  f1: 0.7703\n",
      "Epoch 43 [1.6s]: [all_loss:1.93585, cf_loss:1.96190, cf_reg_loss:0.00506, kge_loss:1.83166, kge_reg_loss 0.00425 ]\n",
      "---epoch 43    train auc: 0.8459  f1: 0.7941    eval auc: 0.8159  f1: 0.7722    test auc: 0.8121  f1: 0.7720\n",
      "Epoch 44 [1.6s]: [all_loss:1.90424, cf_loss:1.92513, cf_reg_loss:0.00515, kge_loss:1.82069, kge_reg_loss 0.00440 ]\n",
      "---epoch 44    train auc: 0.8475  f1: 0.7957    eval auc: 0.8209  f1: 0.7749    test auc: 0.8123  f1: 0.7730\n",
      "Epoch 45 [1.7s]: [all_loss:1.91962, cf_loss:1.96182, cf_reg_loss:0.00528, kge_loss:1.75081, kge_reg_loss 0.00453 ]\n",
      "---epoch 45    train auc: 0.8475  f1: 0.7946    eval auc: 0.8170  f1: 0.7748    test auc: 0.8117  f1: 0.7723\n",
      "Epoch 46 [1.6s]: [all_loss:1.85677, cf_loss:1.87639, cf_reg_loss:0.00529, kge_loss:1.77831, kge_reg_loss 0.00464 ]\n",
      "---epoch 46    train auc: 0.8491  f1: 0.7973    eval auc: 0.8181  f1: 0.7759    test auc: 0.8069  f1: 0.7708\n",
      "Epoch 47 [1.8s]: [all_loss:1.85092, cf_loss:1.88415, cf_reg_loss:0.00535, kge_loss:1.71801, kge_reg_loss 0.00479 ]\n",
      "---epoch 47    train auc: 0.8505  f1: 0.7998    eval auc: 0.8145  f1: 0.7745    test auc: 0.8160  f1: 0.7774\n",
      "Epoch 48 [1.6s]: [all_loss:1.81862, cf_loss:1.85879, cf_reg_loss:0.00541, kge_loss:1.65792, kge_reg_loss 0.00492 ]\n",
      "---epoch 48    train auc: 0.8430  f1: 0.7951    eval auc: 0.8166  f1: 0.7764    test auc: 0.8170  f1: 0.7772\n"
     ]
    }
   ],
   "source": [
    "train_data = data[4]\n",
    "eval_data = data[5]\n",
    "test_data = data[6]\n",
    "for epoch in range(args.n_epochs):\n",
    "    t1 = time()\n",
    "    loss, base_loss,cf_loss,cf_reg_loss, kge_loss, kge_reg_loss = 0., 0., 0., 0.,0.,0.\n",
    "\n",
    "    n_batch = len(train_data) // args.batch_size_cf + 1\n",
    "\n",
    "    if args.use_kge is True:\n",
    "        # using KGE method (knowledge graph embedding).\n",
    "        batch_kg_loss =0.0\n",
    "        batch_kg_reg_loss = 0.0\n",
    "        for idx in range(n_batch):\n",
    "            start = idx*args.batch_size_cf\n",
    "            end = start+args.batch_size_cf\n",
    "            feed_dict = cdata.generate_train_feed_dict(model,start,end) #train\n",
    "            start = end\n",
    "            _, batch_loss,batch_cf_loss,batch_cf_reg_loss,batch_kg_loss,batch_kg_reg_loss = model.train(sess,feed_dict = feed_dict)\n",
    "            loss += batch_loss\n",
    "            kge_loss += batch_kg_loss\n",
    "            cf_loss += batch_cf_loss\n",
    "            cf_reg_loss +=batch_cf_reg_loss\n",
    "            kge_reg_loss += batch_kg_reg_loss\n",
    "\n",
    "    if np.isnan(loss) == True:\n",
    "        print('ERROR: loss@phase2 is nan.')\n",
    "        sys.exit()\n",
    "\n",
    "    show_step = 10\n",
    "    \n",
    "    if (epoch + 1) % show_step != 0:\n",
    "        if args.verbose > 0 and epoch % args.verbose == 0:\n",
    "            \n",
    "            perf_str = 'Epoch %d [%.1fs]: [all_loss:%.5f, cf_loss:%.5f, cf_reg_loss:%.5f, kge_loss:%.5f, kge_reg_loss %.5f ]'\\\n",
    "                            % (epoch, time() - t1,loss,cf_loss,cf_reg_loss, kge_loss,kge_reg_loss)\n",
    "            print(perf_str)\n",
    "            train_auc, train_f1 = cdata.ctr_eval(sess, model, train_data,'train')\n",
    "            eval_auc, eval_f1 = cdata.ctr_eval(sess, model, eval_data,'eval')\n",
    "            test_auc, test_f1 = cdata.ctr_eval(sess, model, test_data,'test')\n",
    "            \n",
    "            print('---epoch %d    train auc: %.4f  f1: %.4f    eval auc: %.4f  f1: %.4f    test auc: %.4f  f1: %.4f'\n",
    "                  % (epoch, train_auc, train_f1, eval_auc, eval_f1, test_auc, test_f1))\n",
    "        continue\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
